{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be897dda",
   "metadata": {},
   "source": [
    "# ü§ñ Supervised Learning for Oil Well Fault Detection\n",
    "\n",
    "## What is Supervised Classification?\n",
    "\n",
    "Supervised learning uses labeled training data to predict categories for new data.\n",
    "\n",
    "**Key Components:**\n",
    "- **Training Data**: Examples with known answers (features + labels)\n",
    "- **Features**: Sensor measurements (input)\n",
    "- **Labels**: Fault types 0-9 (output)\n",
    "- **Model**: Algorithm that learns patterns\n",
    "- **Evaluation**: How well the model performs\n",
    "\n",
    "## Our Problem: 3W Oil Well Classification\n",
    "\n",
    "**Goal**: Predict oil well fault types from sensor data\n",
    "\n",
    "**Input**: Time series from sensors (P-PDG, P-TPT, T-TPT)\n",
    "**Output**: Fault classes (0=normal, 1-9=different faults)\n",
    "**Challenge**: Multiple classes, imbalanced data, high dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce29f117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 3W Dataset for Classification\n",
      "========================================\n",
      "Found 3 folds\n",
      "‚úÖ Data loaded successfully!\n",
      "   Training windows: 70361\n",
      "   Test windows: 14731\n",
      "   Window shape: (300, 4)\n",
      "‚úÖ Data loaded successfully!\n",
      "   Training windows: 70361\n",
      "   Test windows: 14731\n",
      "   Window shape: (300, 4)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LOAD 3W DATASET\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(\"src\")\n",
    "from src import config\n",
    "from src.supervised_classification import (\n",
    "    load_3w_data,\n",
    ")\n",
    "\n",
    "# Load dataset using utility function\n",
    "(train_dfs, train_classes, train_fold_info, test_dfs, test_classes, test_fold_info) = (\n",
    "    load_3w_data(config, verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb8b73a",
   "metadata": {},
   "source": [
    "## Data Loading for Supervised Learning\n",
    "\n",
    "**Key requirements:**\n",
    "- **Labeled data** - Examples with known fault classifications\n",
    "- **Train/test split** - Separate data for training and unbiased evaluation\n",
    "- **Cross-validation** - Multiple data splits for robust performance estimates\n",
    "\n",
    "**3W Dataset structure:**\n",
    "- **Training data** - Used to teach algorithms to recognize patterns\n",
    "- **Test data** - Unseen data to evaluate real-world performance\n",
    "- **Windowed format** - Time series converted to fixed-size sequences\n",
    "- **Class labels** - Fault types (0=normal, 1-8=different fault conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e476fc",
   "metadata": {},
   "source": [
    "# üìä Classification Configuration\n",
    "\n",
    "## Class Selection\n",
    "\n",
    "Choose which fault types to analyze:\n",
    "\n",
    "```python\n",
    "selected_classes = [3, 4, 8]  # Focus on specific faults\n",
    "selected_classes = [1, 2, 3, 4, 5]  # Early fault types  \n",
    "selected_classes = None  # All fault types (exclude normal operation)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "229fa361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Configuration:\n",
      "===================================\n",
      "‚úÖ Data ready: 70361 train, 14731 test windows\n",
      "Selected classes: [3, 4, 8]\n",
      "‚úÖ Fold information: 3 folds detected\n",
      "\n",
      "üìä CLASS DISTRIBUTION BY TEST FOLD\n",
      "=============================================\n",
      "\n",
      "üóÇÔ∏è  fold_1:\n",
      "   Total samples: 4,794\n",
      "   Class 0: 374 samples (7.8%)\n",
      "   Class 1: 126 samples (2.6%)\n",
      "   Class 2: 46 samples (1.0%)\n",
      "   Class 3: 242 samples (5.0%)\n",
      "   Class 4: 136 samples (2.8%)\n",
      "   Class 5: 63 samples (1.3%)\n",
      "   Class 6: 16 samples (0.3%)\n",
      "   Class 7: 3,150 samples (65.7%)\n",
      "   Class 8: 382 samples (8.0%)\n",
      "   Class 9: 259 samples (5.4%)\n",
      "   Selected classes in fold: [3, 4, 8]\n",
      "   Selected classes total: 760 samples (15.9%)\n",
      "\n",
      "üóÇÔ∏è  fold_2:\n",
      "   Total samples: 5,519\n",
      "   Class 0: 374 samples (6.8%)\n",
      "   Class 1: 70 samples (1.3%)\n",
      "   Class 2: 58 samples (1.1%)\n",
      "   Class 3: 238 samples (4.3%)\n",
      "   Class 4: 133 samples (2.4%)\n",
      "   Class 5: 283 samples (5.1%)\n",
      "   Class 6: 45 samples (0.8%)\n",
      "   Class 7: 3,316 samples (60.1%)\n",
      "   Class 8: 820 samples (14.9%)\n",
      "   Class 9: 182 samples (3.3%)\n",
      "   Selected classes in fold: [3, 4, 8]\n",
      "   Selected classes total: 1,191 samples (21.6%)\n",
      "\n",
      "üóÇÔ∏è  fold_3:\n",
      "   Total samples: 4,418\n",
      "   Class 0: 362 samples (8.2%)\n",
      "   Class 1: 94 samples (2.1%)\n",
      "   Class 2: 113 samples (2.6%)\n",
      "   Class 3: 220 samples (5.0%)\n",
      "   Class 4: 136 samples (3.1%)\n",
      "   Class 5: 171 samples (3.9%)\n",
      "   Class 6: 5 samples (0.1%)\n",
      "   Class 7: 1,607 samples (36.4%)\n",
      "   Class 8: 1,608 samples (36.4%)\n",
      "   Class 9: 102 samples (2.3%)\n",
      "   Selected classes in fold: [3, 4, 8]\n",
      "   Selected classes total: 1,964 samples (44.5%)\n",
      "\n",
      "üìà CROSS-FOLD SUMMARY\n",
      "=========================\n",
      "\n",
      "Class   fold_1      fold_2      fold_3      Total\n",
      "--------------------------------------------------------\n",
      "0       374         374         362         1,110       \n",
      "1       126         70          94          290         \n",
      "2       46          58          113         217         \n",
      "3       242         238         220         700         \n",
      "4       136         133         136         405         \n",
      "5       63          283         171         517         \n",
      "6       16          45          5           66          \n",
      "7       3,150       3,316       1,607       8,073       \n",
      "8       382         820         1,608       2,810       \n",
      "9       259         182         102         543         \n",
      "--------------------------------------------------------\n",
      "Total   4,794       5,519       4,418       14,731      \n",
      "\n",
      "üéØ SELECTED CLASSES SUMMARY\n",
      "==============================\n",
      "Class 3: 700 samples (4.8% of total)\n",
      "          Folds: fold_1: 242, fold_2: 238, fold_3: 220\n",
      "Class 4: 405 samples (2.7% of total)\n",
      "          Folds: fold_1: 136, fold_2: 133, fold_3: 136\n",
      "Class 8: 2,810 samples (19.1% of total)\n",
      "          Folds: fold_1: 382, fold_2: 820, fold_3: 1,608\n",
      "\n",
      "‚úÖ Class distribution analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURE CLASSIFICATION\n",
    "# ============================================================\n",
    "\n",
    "from src.supervised_classification import validate_configuration, print_class_distribution_analysis\n",
    "\n",
    "# Configuration\n",
    "selected_classes = [3, 4, 8]  # Choose fault types to analyze\n",
    "balance_test = False  # Keep original test distribution\n",
    "min_test_samples_per_class = 300\n",
    "\n",
    "print(\"Classification Configuration:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Verify data availability\n",
    "if not (train_dfs and test_dfs):\n",
    "    print(\"‚ùå No data available. Run the data loading cell first.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Data ready: {len(train_dfs)} train, {len(test_dfs)} test windows\")\n",
    "\n",
    "    # Validate configuration\n",
    "    config_valid = validate_configuration(selected_classes, test_classes, verbose=True)\n",
    "\n",
    "    # Check fold information\n",
    "    fold_available = test_fold_info is not None and len(test_fold_info) == len(test_dfs)\n",
    "    if fold_available:\n",
    "        unique_folds = sorted(set(test_fold_info))\n",
    "        print(f\"‚úÖ Fold information: {len(unique_folds)} folds detected\")\n",
    "        \n",
    "        # Print comprehensive class distribution analysis using module function\n",
    "        print_class_distribution_analysis(\n",
    "            test_classes=test_classes,\n",
    "            test_fold_info=test_fold_info,\n",
    "            selected_classes=selected_classes,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No fold information available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9947d92",
   "metadata": {},
   "source": [
    "## Multi-Algorithm Training Strategy\n",
    "\n",
    "**Algorithm categories:**\n",
    "- **Tree-based** - can be (depends on the algoritm and application): Interpretable, handle non-linear patterns , robust to outliers\n",
    "- **Support Vector Machines** - Effective for high-dimensional data, kernel flexibility\n",
    "- **Neural Networks** - Can learn complex patterns, require more data and tuning\n",
    "\n",
    "**Evaluation strategy:**\n",
    "- **Cross-validation** - Test on multiple data splits for robust estimates\n",
    "- **Multiple metrics** - Accuracy, precision, recall, F1-score\n",
    "- **Computational efficiency** - Training time and memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956fe609",
   "metadata": {},
   "source": [
    "## Tree-Based Algorithms Theory\n",
    "\n",
    "**Decision Trees:**\n",
    "- **How they work** - Split data based on feature values to separate classes\n",
    "- **Interpretability** - Easy to understand and visualize decision paths\n",
    "- **Strengths** - Handle non-linear patterns, robust to outliers\n",
    "- **Weaknesses** - Prone to overfitting, unstable (small data changes = different tree)\n",
    "\n",
    "**Random Forest:**\n",
    "- **Ensemble method** - Combines many decision trees for better performance\n",
    "- **Bootstrap aggregating** - Each tree trained on random data subset\n",
    "- **Feature randomness** - Each split considers random subset of features\n",
    "- **Benefits** - Reduces overfitting, more stable, provides feature importance\n",
    "\n",
    "**For oil well data:** Good at capturing threshold-based rules (e.g., \"if pressure > X and temperature < Y, then fault type Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DECISION TREES ANALYSIS - BY FOLD/FOLDER\n",
    "# ============================================================\n",
    "\n",
    "from src.supervised_classification import tree_based_fold_analysis, print_tree_analysis_results\n",
    "\n",
    "# Check if data is available\n",
    "if not (train_dfs and test_dfs):\n",
    "    print(\"‚ùå No data available. Run the data loading cell first.\")\n",
    "elif not fold_available:\n",
    "    print(\"‚ùå No fold information available. Cannot analyze by folder.\")\n",
    "else:\n",
    "    print(\"üå≥ TRAINING TREE-BASED MODELS - BY FOLD/FOLDER\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Run tree-based analysis for each fold (verbose=False to avoid duplicates)\n",
    "    tree_analysis_results = tree_based_fold_analysis(\n",
    "        train_dfs=train_dfs,\n",
    "        train_classes=train_classes,\n",
    "        test_dfs=test_dfs,\n",
    "        test_classes=test_classes,\n",
    "        train_fold_info=train_fold_info,\n",
    "        test_fold_info=test_fold_info,\n",
    "        selected_classes=selected_classes,\n",
    "        balance_classes=True,\n",
    "        balance_strategy=\"combined\",\n",
    "        max_samples_per_class=1000,\n",
    "        verbose=False  # Set to False to avoid duplicate output\n",
    "    )\n",
    "    \n",
    "    # Print detailed analysis using module function\n",
    "    print_tree_analysis_results(tree_analysis_results, selected_classes)\n",
    "    \n",
    "    # Store for potential later use\n",
    "    tree_fold_analysis = tree_analysis_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da46019e",
   "metadata": {},
   "source": [
    "# ‚ö° Support Vector Machines (SVM)\n",
    "\n",
    "**How SVM Works**: Finds the best boundary (hyperplane) to separate different classes with maximum margin.\n",
    "\n",
    "**Key Advantages**:\n",
    "- Effective with high-dimensional data (many features)\n",
    "- Memory efficient (uses only support vectors)\n",
    "- Different kernels for different data patterns\n",
    "\n",
    "**For Oil Well Data**: Handles flattened time series well, RBF kernel captures non-linear sensor patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef0fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SVM ANALYSIS - BY FOLD/FOLDER\n",
    "# ============================================================\n",
    "\n",
    "from src.supervised_classification import svm_based_fold_analysis, print_svm_analysis_results\n",
    "\n",
    "# Check if data is available\n",
    "if not (train_dfs and test_dfs):\n",
    "    print(\"‚ùå No data available. Run the data loading cell first.\")\n",
    "elif not fold_available:\n",
    "    print(\"‚ùå No fold information available. Cannot analyze by folder.\")\n",
    "else:\n",
    "    print(\"‚ö° TRAINING SVM MODELS - BY FOLD/FOLDER\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Run SVM-based analysis for each fold (verbose=False to avoid duplicates)\n",
    "    svm_analysis_results = svm_based_fold_analysis(\n",
    "        train_dfs=train_dfs,\n",
    "        train_classes=train_classes,\n",
    "        test_dfs=test_dfs,\n",
    "        test_classes=test_classes,\n",
    "        train_fold_info=train_fold_info,\n",
    "        test_fold_info=test_fold_info,\n",
    "        selected_classes=selected_classes,\n",
    "        balance_classes=True,\n",
    "        balance_strategy=\"combined\",\n",
    "        max_samples_per_class=1000,\n",
    "        verbose=False  # Set to False to avoid duplicate output\n",
    "    )\n",
    "    \n",
    "    # Print detailed analysis using module function\n",
    "    print_svm_analysis_results(svm_analysis_results, selected_classes)\n",
    "    \n",
    "    # Store for potential later use\n",
    "    svm_fold_analysis = svm_analysis_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b91df2",
   "metadata": {},
   "source": [
    "# üß† Neural Networks\n",
    "\n",
    "**How Neural Networks Work**: Interconnected layers of neurons that learn complex patterns through training.\n",
    "\n",
    "**Key Components**:\n",
    "- **Neurons**: Process input and apply activation functions\n",
    "- **Layers**: Input ‚Üí Hidden ‚Üí Output\n",
    "- **Backpropagation**: Adjusts weights based on errors\n",
    "\n",
    "**Types Used**:\n",
    "- **Simple NN**: Fast baseline, good for simple patterns\n",
    "- **Deep NN**: Multiple hidden layers for complex patterns\n",
    "- **Regularized NN**: Uses dropout to prevent overfitting\n",
    "\n",
    "**For Time Series**: Can automatically discover patterns in sensor data and handle complex temporal relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b809c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# NEURAL NETWORKS ANALYSIS - BY FOLD/FOLDER\n",
    "# ============================================================\n",
    "\n",
    "from src.supervised_classification import neural_network_based_fold_analysis, print_neural_network_analysis_results\n",
    "\n",
    "# Check if data is available\n",
    "if not (train_dfs and test_dfs):\n",
    "    print(\"‚ùå No data available. Run the data loading cell first.\")\n",
    "elif not fold_available:\n",
    "    print(\"‚ùå No fold information available. Cannot analyze by folder.\")\n",
    "else:\n",
    "    print(\"üß† TRAINING NEURAL NETWORK MODELS - BY FOLD/FOLDER\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Run neural network-based analysis for each fold (verbose=False to avoid duplicates)\n",
    "    nn_analysis_results = neural_network_based_fold_analysis(\n",
    "        train_dfs=train_dfs,\n",
    "        train_classes=train_classes,\n",
    "        test_dfs=test_dfs,\n",
    "        test_classes=test_classes,\n",
    "        train_fold_info=train_fold_info,\n",
    "        test_fold_info=test_fold_info,\n",
    "        selected_classes=selected_classes,\n",
    "        balance_classes=True,\n",
    "        balance_strategy=\"combined\",\n",
    "        max_samples_per_class=1000,\n",
    "        verbose=False  # Set to False to avoid duplicate output\n",
    "    )\n",
    "    \n",
    "    # Print detailed analysis using module function\n",
    "    print_neural_network_analysis_results(nn_analysis_results, selected_classes)\n",
    "    \n",
    "    # Store for potential later use\n",
    "    nn_fold_analysis = nn_analysis_results\n",
    "\n",
    "# Print neural network architectures for reference\n",
    "from src.supervised_classification import print_neural_network_architectures\n",
    "print_neural_network_architectures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b6c9c6",
   "metadata": {},
   "source": [
    "## Comprehensive Results Analysis\n",
    "\n",
    "**Visualization goals:**\n",
    "- **Algorithm comparison** - Which performs best for oil well fault detection?\n",
    "- **Overfitting assessment** - Are models generalizing well to new data?\n",
    "- **Performance patterns** - What insights can guide future improvements?\n",
    "\n",
    "**Key analyses:**\n",
    "1. **Test accuracy ranking** - Real-world performance comparison\n",
    "2. **Train vs test gap** - Generalization assessment\n",
    "- **High train, low test** - Overfitting (model too complex)\n",
    "- **Similar train/test** - Good generalization\n",
    "- **Low both** - Underfitting (model too simple or insufficient data)\n",
    "3. **Algorithm categories** - Tree-based vs SVM vs Neural Network strengths\n",
    "4. **Statistical significance** - Are performance differences meaningful?\n",
    "\n",
    "**Decision-making criteria:**\n",
    "- **Primary metric** - Test accuracy (real-world performance)\n",
    "- **Secondary factors** - Training time, interpretability, stability\n",
    "- **Domain constraints** - Real-time requirements, maintenance complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca9f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN CLASSIFICATION MODELS\n",
    "# ============================================================\n",
    "\n",
    "from src.supervised_classification import enhanced_fold_analysis\n",
    "from tqdm import tqdm\n",
    "\n",
    "if not (train_dfs and test_dfs):\n",
    "    print(\"‚ùå No data available for training\")\n",
    "    supervised_classifier = None\n",
    "else:\n",
    "    print(\"üöÄ Training Classification Models\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    models_to_train = [\n",
    "        \"Decision Tree\",\n",
    "        \"Random Forest\",\n",
    "        \"Linear SVM\",\n",
    "        \"RBF SVM\",\n",
    "        \"Simple Neural Network\",\n",
    "        \"Deep Neural Network\",\n",
    "        \"Regularized Neural Network\",\n",
    "    ]\n",
    "\n",
    "    print(f\"Training {len(models_to_train)} algorithms...\")\n",
    "\n",
    "    # Train models with progress bar\n",
    "    with tqdm(total=len(models_to_train), desc=\"Training\", unit=\"model\") as pbar:\n",
    "        try:\n",
    "            classifier = enhanced_fold_analysis(\n",
    "                train_dfs=train_dfs,\n",
    "                train_classes=train_classes,\n",
    "                test_dfs=test_dfs,\n",
    "                test_classes=test_classes,\n",
    "                train_fold_info=(train_fold_info if fold_available else None),\n",
    "                test_fold_info=(test_fold_info if fold_available else None),\n",
    "                balance_classes=True,\n",
    "                balance_strategy=\"combined\",\n",
    "                max_samples_per_class=1000,\n",
    "                balance_test=balance_test,\n",
    "                min_test_samples_per_class=min_test_samples_per_class,\n",
    "                selected_classes=selected_classes,\n",
    "                verbose=False,\n",
    "            )\n",
    "            pbar.update(len(models_to_train))\n",
    "            supervised_classifier = classifier\n",
    "            print(\"‚úÖ Training completed successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Training failed: {e}\")\n",
    "            supervised_classifier = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa04f286",
   "metadata": {},
   "source": [
    "## Understanding the `enhanced_fold_analysis` Method\n",
    "\n",
    "**What just happened?** The `enhanced_fold_analysis` method performed a comprehensive machine learning experiment. Here's exactly what it did:\n",
    "\n",
    "> **üìä Important Note**: The following results show the **overall performance summary** averaged across all folds. Individual algorithm analyses below focus on **single-fold results** for detailed examination. At the end of this notebook, we'll compare **all folds together** to assess model consistency and stability.\n",
    "\n",
    "### 1. **Class Balancing Strategy**\n",
    "- **Problem detected**: Class imbalance (Class 3: ~12k samples, Class 4: ~2k samples, Class 8: ~8k samples)\n",
    "- **Solution applied**: \"Combined\" balancing strategy\n",
    "  - **Step 1**: Reduce majority classes to match minority class (downsampling)\n",
    "  - **Step 2**: Augment minority class to match others (upsampling/SMOTE)\n",
    "- **Result**: All classes now have equal samples (~8k-9k each)\n",
    "\n",
    "### 2. **Cross-Validation Process**\n",
    "- **3 folds processed**: Each fold had slightly different class distributions\n",
    "- **Consistent balancing**: Applied same strategy to each fold independently\n",
    "- **Avoided data leakage**: Balancing done separately for each train/test split\n",
    "\n",
    "### 3. **Algorithm Training**\n",
    "- **7 algorithms trained** on each fold:\n",
    "  - Decision Tree, Random Forest (tree-based)\n",
    "  - Linear SVM, RBF SVM (support vector machines)\n",
    "  - Simple NN, Deep NN, Regularized NN (neural networks)\n",
    "- **Progress tracking**: 100% completion in ~36 seconds\n",
    "\n",
    "### 4. **Performance Evaluation**\n",
    "- **Multiple metrics calculated**: Train accuracy, test accuracy, training time, overfitting\n",
    "- **Fastest algorithm**: RBF SVM (0.045 seconds)\n",
    "- **Best generalization**: RBF SVM (lowest overfitting gap)\n",
    "\n",
    "### 5. **Output Analysis**\n",
    "- **Detailed classification report** provided for best model (Random Forest)\n",
    "- **Per-class performance**: Precision, recall, F1-score for each fault type\n",
    "- **Overall statistics**: Macro and weighted averages\n",
    "\n",
    "**Key insight**: This single method call performed what would normally require hundreds of lines of code - data preprocessing, multiple algorithm training, cross-validation, and comprehensive evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b65002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESULTS SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "if supervised_classifier is not None and hasattr(supervised_classifier, \"results\"):\n",
    "    results = supervised_classifier.results\n",
    "    # Best model\n",
    "    best_model = max(results, key=lambda x: x[\"test_accuracy\"])\n",
    "\n",
    "# ============================================================\n",
    "# CROSS-FOLD VALIDATION RESULTS\n",
    "# ============================================================\n",
    "\n",
    "if (\n",
    "    supervised_classifier is not None\n",
    "    and hasattr(supervised_classifier, \"fold_results\")\n",
    "    and supervised_classifier.fold_results\n",
    "):\n",
    "\n",
    "    print(\"üìà CROSS-FOLD VALIDATION\")\n",
    "    print(\"=\" * 35)\n",
    "\n",
    "    fold_results = supervised_classifier.fold_results\n",
    "    fold_names = sorted(fold_results.keys())\n",
    "\n",
    "    if fold_names and results:\n",
    "        model_names = [r[\"model_name\"] for r in results]\n",
    "\n",
    "        # Table header\n",
    "        header = f\"{'Fold':<12}\"\n",
    "        for model_name in model_names:\n",
    "            short_name = model_name.replace(\" Neural Network\", \" NN\").replace(\n",
    "                \"Random Forest\", \"RF\"\n",
    "            )[:10]\n",
    "            header += f\"{short_name:<12}\"\n",
    "        print(f\"\\n{header}\")\n",
    "        print(\"-\" * len(header))\n",
    "\n",
    "        # Fold results\n",
    "        for fold in fold_names:\n",
    "            row = f\"{fold:<12}\"\n",
    "            for model_name in model_names:\n",
    "                accuracy = fold_results[fold].get(model_name, 0.0)\n",
    "                row += f\"{accuracy:<12.3f}\"\n",
    "            print(row)\n",
    "\n",
    "        # Calculate averages\n",
    "        print(\"-\" * len(header))\n",
    "        avg_row = f\"{'Average':<12}\"\n",
    "        for model_name in model_names:\n",
    "            accuracies = [\n",
    "                fold_results[fold].get(model_name, 0.0) for fold in fold_names\n",
    "            ]\n",
    "            avg_accuracy = np.mean(accuracies) if accuracies else 0.0\n",
    "            avg_row += f\"{avg_accuracy:<12.3f}\"\n",
    "        print(avg_row)\n",
    "\n",
    "        # Store for later analysis\n",
    "        fold_accuracies = fold_results\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cross-fold validation not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0e926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìà RESULTS VISUALIZATION AND ANALYSIS - CROSS-FOLD AVERAGES\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.supervised_classification import analyze_results_by_category\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "if (\n",
    "    \"supervised_classifier\" in locals()\n",
    "    and supervised_classifier is not None\n",
    "    and \"fold_accuracies\" in locals()\n",
    "    and fold_accuracies\n",
    "):\n",
    "\n",
    "    print(\"üéØ GENERATING COMPREHENSIVE VISUALIZATIONS - CROSS-FOLD AVERAGES\")\n",
    "    print(\"=\" * 65)\n",
    "\n",
    "    # ============================================================\n",
    "    # EXTRACT AVERAGED FOLD RESULTS\n",
    "    # ============================================================\n",
    "\n",
    "    # Get model names from original results for consistency\n",
    "    original_results = supervised_classifier.results\n",
    "    model_names = [r[\"model_name\"] for r in original_results]\n",
    "\n",
    "    # Calculate averaged results across all folds\n",
    "    fold_names = sorted(fold_accuracies.keys())\n",
    "    averaged_results = []\n",
    "\n",
    "    for model_name in model_names:\n",
    "        # Get accuracies across all folds for this model\n",
    "        fold_accs = [fold_accuracies[fold].get(model_name, 0.0) for fold in fold_names]\n",
    "        avg_test_acc = np.mean(fold_accs) if fold_accs else 0.0\n",
    "\n",
    "        # Find corresponding original result for train accuracy and time\n",
    "        original_result = next(\n",
    "            (r for r in original_results if r[\"model_name\"] == model_name), None\n",
    "        )\n",
    "        train_acc = original_result[\"train_accuracy\"] if original_result else 0.0\n",
    "        train_time = original_result[\"training_time\"] if original_result else 0.0\n",
    "\n",
    "        averaged_results.append(\n",
    "            {\n",
    "                \"model_name\": model_name,\n",
    "                \"test_accuracy\": avg_test_acc,\n",
    "                \"train_accuracy\": train_acc,  # Note: this is from single fold\n",
    "                \"training_time\": train_time,\n",
    "                \"fold_variance\": np.std(fold_accs) if len(fold_accs) > 1 else 0.0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Extract data for plotting - NOW USING CROSS-FOLD AVERAGES\n",
    "    test_accuracies = [r[\"test_accuracy\"] for r in averaged_results]\n",
    "    train_accuracies = [r[\"train_accuracy\"] for r in averaged_results]\n",
    "    fold_variances = [r[\"fold_variance\"] for r in averaged_results]\n",
    "\n",
    "    print(f\"üìä Using averaged results across {len(fold_names)} folds\")\n",
    "    print(f\"   ‚Ä¢ Fold variance indicates consistency across different data splits\")\n",
    "\n",
    "    # ============================================================\n",
    "    # 1. PERFORMANCE COMPARISON BAR CHART\n",
    "    # ============================================================\n",
    "\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(\n",
    "        \"Supervised Learning Classification Results - Cross-Fold Averages\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    # Bar chart of test accuracies (AVERAGED ACROSS FOLDS)\n",
    "    bars1 = ax1.bar(\n",
    "        range(len(model_names)), test_accuracies, alpha=0.8, color=\"steelblue\"\n",
    "    )\n",
    "    ax1.set_title(\"Average Test Accuracy Across All Folds\", fontweight=\"bold\")\n",
    "    ax1.set_ylabel(\"Test Accuracy (Average)\")\n",
    "    ax1.set_xticks(range(len(model_names)))\n",
    "    ax1.set_xticklabels(model_names, rotation=45, ha=\"right\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for i, (bar, acc) in enumerate(zip(bars1, test_accuracies)):\n",
    "        ax1.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            bar.get_height() + 0.01,\n",
    "            f\"{acc:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    # ============================================================\n",
    "    # 2. FOLD CONSISTENCY ANALYSIS (VARIANCE)\n",
    "    # ============================================================\n",
    "\n",
    "    # Color by variance (low variance = green, high variance = red)\n",
    "    colors_variance = [\n",
    "        \"green\" if v < 0.02 else \"orange\" if v < 0.05 else \"red\" for v in fold_variances\n",
    "    ]\n",
    "\n",
    "    bars2 = ax2.bar(\n",
    "        range(len(model_names)), fold_variances, alpha=0.8, color=colors_variance\n",
    "    )\n",
    "    ax2.set_title(\"Cross-Fold Consistency (Lower = Better)\", fontweight=\"bold\")\n",
    "    ax2.set_ylabel(\"Standard Deviation Across Folds\")\n",
    "    ax2.set_xticks(range(len(model_names)))\n",
    "    ax2.set_xticklabels(model_names, rotation=45, ha=\"right\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, (bar, var) in enumerate(zip(bars2, fold_variances)):\n",
    "        ax2.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            bar.get_height() + 0.002,\n",
    "            f\"{var:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "    # ============================================================\n",
    "    # 3. OVERFITTING ANALYSIS (Note: Train is from single fold)\n",
    "    # ============================================================\n",
    "\n",
    "    # Calculate overfitting (train - averaged test accuracy)\n",
    "    overfitting = [\n",
    "        train - test for train, test in zip(train_accuracies, test_accuracies)\n",
    "    ]\n",
    "    colors = [\n",
    "        \"red\" if x > 0.1 else \"orange\" if x > 0.05 else \"green\" for x in overfitting\n",
    "    ]\n",
    "\n",
    "    bars4 = ax3.bar(range(len(model_names)), overfitting, alpha=0.8, color=colors)\n",
    "    ax3.set_title(\"Overfitting Analysis (Train - Avg Test)\", fontweight=\"bold\")\n",
    "    ax3.set_ylabel(\"Accuracy Difference\")\n",
    "    ax3.set_xticks(range(len(model_names)))\n",
    "    ax3.set_xticklabels(model_names, rotation=45, ha=\"right\")\n",
    "    ax3.axhline(\n",
    "        y=0.1, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"High Overfitting (>10%)\"\n",
    "    )\n",
    "    ax3.axhline(\n",
    "        y=0.05,\n",
    "        color=\"orange\",\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.7,\n",
    "        label=\"Moderate Overfitting (>5%)\",\n",
    "    )\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # ============================================================\n",
    "    # 4. ALGORITHM RANKING BY AVERAGED PERFORMANCE\n",
    "    # ============================================================\n",
    "\n",
    "    # Sort by averaged test accuracy\n",
    "    sorted_results = sorted(\n",
    "        zip(model_names, test_accuracies, fold_variances),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True,\n",
    "    )\n",
    "    sorted_names, sorted_accs, sorted_vars = zip(*sorted_results)\n",
    "\n",
    "    colors_ranking = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(sorted_names)))\n",
    "    bars5 = ax4.barh(\n",
    "        range(len(sorted_names)), sorted_accs, alpha=0.8, color=colors_ranking\n",
    "    )\n",
    "    ax4.set_title(\"Algorithm Ranking (Averaged Test Accuracy)\", fontweight=\"bold\")\n",
    "    ax4.set_xlabel(\"Average Test Accuracy\")\n",
    "    ax4.set_yticks(range(len(sorted_names)))\n",
    "    ax4.set_yticklabels(sorted_names)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels with variance info\n",
    "    for i, (bar, acc, var) in enumerate(zip(bars5, sorted_accs, sorted_vars)):\n",
    "        ax4.text(\n",
    "            bar.get_width() + 0.005,\n",
    "            bar.get_y() + bar.get_height() / 2.0,\n",
    "            f\"{acc:.3f} (¬±{var:.3f})\",\n",
    "            ha=\"left\",\n",
    "            va=\"center\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ============================================================\n",
    "    # 5. DETAILED RESULTS TABLE - CROSS-FOLD AVERAGES\n",
    "    # ============================================================\n",
    "\n",
    "    print(\"\\nüìã DETAILED RESULTS TABLE - CROSS-FOLD AVERAGES\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Create DataFrame for better visualization\n",
    "    df_results = pd.DataFrame(\n",
    "        {\n",
    "            \"Algorithm\": model_names,\n",
    "            \"Avg Test Accuracy\": [f\"{acc:.3f}\" for acc in test_accuracies],\n",
    "            \"Fold Std Dev\": [f\"{var:.3f}\" for var in fold_variances],\n",
    "            \"Consistency\": [\n",
    "                \"‚úÖ High\" if var < 0.02 else \"üî∂ Medium\" if var < 0.05 else \"‚ö†Ô∏è Low\"\n",
    "                for var in fold_variances\n",
    "            ],\n",
    "            \"Overfitting\": [f\"{diff:.3f}\" for diff in overfitting],\n",
    "            \"Generalization\": [\n",
    "                \"‚ö†Ô∏è High\" if diff > 0.1 else \"üî∂ Moderate\" if diff > 0.05 else \"‚úÖ Good\"\n",
    "                for diff in overfitting\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Sort by averaged test accuracy\n",
    "    df_results = df_results.sort_values(\"Avg Test Accuracy\", ascending=False)\n",
    "    print(df_results.to_string(index=False))\n",
    "\n",
    "    # ============================================================\n",
    "    # 6. SUMMARY STATISTICS - CROSS-FOLD AVERAGES\n",
    "    # ============================================================\n",
    "\n",
    "    print(f\"\\nüìä CROSS-FOLD SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\n",
    "        f\"üéØ Best Algorithm: {sorted_names[0]} ({sorted_accs[0]:.3f} ¬± {sorted_vars[0]:.3f})\"\n",
    "    )\n",
    "    print(f\"üìà Average Test Accuracy: {np.mean(test_accuracies):.3f}\")\n",
    "    print(\n",
    "        f\"üìâ Worst Algorithm: {sorted_names[-1]} ({sorted_accs[-1]:.3f} ¬± {sorted_vars[-1]:.3f})\"\n",
    "    )\n",
    "    print(f\"üîÑ Accuracy Range: {max(test_accuracies) - min(test_accuracies):.3f}\")\n",
    "\n",
    "    # Count consistency levels\n",
    "    high_consistency = sum(1 for x in fold_variances if x < 0.02)\n",
    "    medium_consistency = sum(1 for x in fold_variances if 0.02 <= x < 0.05)\n",
    "    low_consistency = sum(1 for x in fold_variances if x >= 0.05)\n",
    "\n",
    "    print(f\"\\nüîÑ CROSS-FOLD CONSISTENCY ANALYSIS:\")\n",
    "    print(\n",
    "        f\"   ‚úÖ High Consistency: {high_consistency}/{len(averaged_results)} algorithms\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   üî∂ Medium Consistency: {medium_consistency}/{len(averaged_results)} algorithms\"\n",
    "    )\n",
    "    print(f\"   ‚ö†Ô∏è Low Consistency: {low_consistency}/{len(averaged_results)} algorithms\")\n",
    "\n",
    "    # Most consistent algorithm\n",
    "    most_consistent = min(zip(model_names, fold_variances), key=lambda x: x[1])\n",
    "    print(\n",
    "        f\"\\nüèÜ Most Consistent Algorithm: {most_consistent[0]} (std: {most_consistent[1]:.3f})\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüéì CROSS-FOLD LEARNING INSIGHTS:\")\n",
    "    print(f\"   ‚Ä¢ Results averaged across {len(fold_names)} cross-validation folds\")\n",
    "    print(f\"   ‚Ä¢ Consistency measured by standard deviation across folds\")\n",
    "    print(f\"   ‚Ä¢ Best algorithm combines high accuracy with low variance\")\n",
    "    print(f\"   ‚Ä¢ Classes analyzed: {selected_classes}\")\n",
    "\n",
    "    print(\n",
    "        f\"\\n‚úÖ Cross-fold visualization complete! Robust performance estimates ready.\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No cross-fold results available.\")\n",
    "    print(\"üí° Please run all the previous cells first:\")\n",
    "    print(\"   1. Data loading and configuration\")\n",
    "    print(\"   2. Model training and validation\")\n",
    "    print(\"   3. Cross-fold validation analysis\")\n",
    "    print(\"\\nThen re-run this cell to see cross-fold averaged visualizations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c3f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
