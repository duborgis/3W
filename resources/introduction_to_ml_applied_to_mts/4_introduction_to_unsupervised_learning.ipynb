{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be897dda",
   "metadata": {},
   "source": [
    "# 📚 Unsupervised Learning and Novelty Detection (5 minutes)\n",
    "\n",
    "### What is Unsupervised Learning?\n",
    "\n",
    "**Unsupervised Learning** is a machine learning approach where we analyze data without labeled examples, seeking to discover hidden patterns, structures, or anomalies in the data.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Unlabeled Data**: Input data without target labels\n",
    "2. **Pattern Discovery**: Finding hidden structures in data\n",
    "3. **Anomaly Detection**: Identifying unusual or outlier patterns\n",
    "4. **Dimensionality Reduction**: Learning compact representations\n",
    "5. **Reconstruction**: Learning to recreate input data\n",
    "\n",
    "### Our Novelty Detection Problem: 3W Oil Well Anomaly Detection\n",
    "\n",
    "- **Objective**: Detect abnormal oil well operational states using only normal operation data\n",
    "- **Training Data**: Only class 0 (normal operation) sensor measurements\n",
    "- **Detection Target**: Identify when sensor patterns deviate from normal behavior\n",
    "- **Challenge**: Learn normal patterns to detect any deviation as potential fault\n",
    "\n",
    "### Why Novelty Detection Matters in Oil Wells:\n",
    "- **Early Warning System**: Detect problems before they escalate\n",
    "- **Unsupervised Monitoring**: No need for labeled fault examples\n",
    "- **Operational Safety**: Continuous monitoring of normal vs abnormal states\n",
    "- **Preventive Maintenance**: Identify subtle deviations before major failures\n",
    "\n",
    "### Autoencoder-Based Novelty Detection Approach:\n",
    "- **Training Phase**: Learn to reconstruct only normal operation patterns (class 0)\n",
    "- **Detection Phase**: High reconstruction error indicates potential anomaly\n",
    "- **Threshold**: Statistical approach (mean + 3×std) for anomaly scoring\n",
    "- **Evaluation**: Test how well it distinguishes normal vs fault classes\n",
    "\n",
    "### Problem Characteristics:\n",
    "- **Normal-Only Training**: Learn patterns from class 0 data exclusively\n",
    "- **Time Series**: Sequential sensor measurements requiring LSTM architecture\n",
    "- **High Dimensional**: Many sensors × time steps = complex patterns\n",
    "- **Reconstruction-Based**: Anomalies have higher reconstruction errors\n",
    "- **Threshold-Based**: Statistical approach for anomaly detection\n",
    "\n",
    "Let's explore LSTM autoencoders for oil well novelty detection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce29f117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 3W Dataset for Autoencoder Novelty Detection\n",
      "============================================================\n",
      "Importing modules... ✅\n",
      "Anomaly Class Configuration: [3, 4, 8]\n",
      "Optimization Settings:\n",
      "   • Single fold loading: True\n",
      "   • Target fold: fold_1\n",
      "   • Sampling enabled: True\n",
      "   • Max normal samples: 2000\n",
      "   • Max anomaly samples: 1000\n",
      "   • Selected anomaly classes: [3, 4, 8]\n",
      "\n",
      "Initializing data persistence... ✅\n",
      "Using format: pickle\n",
      "Checking windowed directory: processed_data\\cv_splits\\windowed... ✅\n",
      "Looking for fold directories... ✅ Found 3 folds: ['fold_1', 'fold_2', 'fold_3']\n",
      "Using single fold: fold_1\n",
      "\n",
      "Processing fold_1 (1/1)...\n",
      "   Loading train data... ✅\n",
      "Anomaly Class Configuration: [3, 4, 8]\n",
      "Optimization Settings:\n",
      "   • Single fold loading: True\n",
      "   • Target fold: fold_1\n",
      "   • Sampling enabled: True\n",
      "   • Max normal samples: 2000\n",
      "   • Max anomaly samples: 1000\n",
      "   • Selected anomaly classes: [3, 4, 8]\n",
      "\n",
      "Initializing data persistence... ✅\n",
      "Using format: pickle\n",
      "Checking windowed directory: processed_data\\cv_splits\\windowed... ✅\n",
      "Looking for fold directories... ✅ Found 3 folds: ['fold_1', 'fold_2', 'fold_3']\n",
      "Using single fold: fold_1\n",
      "\n",
      "Processing fold_1 (1/1)...\n",
      "   Loading train data... ✅ (168051 windows)\n",
      "      Current totals: 2000 normal, 1000 anomaly\n",
      "      Sampling limits reached - stopping early\n",
      "   Sampling complete - sufficient data collected\n",
      "\n",
      "✅ Successfully loaded and separated windowed data!\n",
      "Normal operation windows (class 0): 2000\n",
      "Anomaly windows (classes [3, 4, 8]): 1000\n",
      "Loading time: 11.140 seconds\n",
      "Files processed: 1\n",
      "\n",
      "Processing sample windows... ✅\n",
      "\n",
      "Sample Normal Window (Window #1):\n",
      "   • Shape: (300, 4)\n",
      "   • Class: 0 (Normal Operation)\n",
      "   • Features: ['P-PDG_scaled', 'P-TPT_scaled', 'T-TPT_scaled', 'class']\n",
      "\n",
      "Normal Operation Data (Class 0):\n",
      "   • Total windows: 2000\n",
      "   • Usage: Autoencoder training\n",
      "\n",
      "Anomaly Data (Selected Classes [3, 4, 8]):\n",
      "   • Class 3: 1000 windows\n",
      "   • Total anomaly windows: 1000\n",
      "   • Usage: Anomaly detection testing\n",
      "\n",
      "Performance Summary:\n",
      "   • Total execution time: 14.475 seconds\n",
      "   • Data loading time: 11.140 seconds\n",
      "   • File format: pickle\n",
      "   • Folds processed: 1\n",
      "\n",
      "Dataset Summary for Novelty Detection:\n",
      "   • Normal training data: 2000 windows\n",
      "   • Anomaly test data: 1000 windows\n",
      "   • Window dimensions: (300, 4)\n",
      "   • Selected anomaly classes: [np.str_('3')]\n",
      "\n",
      "Configuration Notes:\n",
      "   • Sampling enabled for faster processing\n",
      "   • Only classes [3, 4, 8] used as anomalies\n",
      "   • To use full dataset: Set ENABLE_SAMPLING = False\n",
      "   • To use all folds: Set USE_SINGLE_FOLD = False\n",
      "✅ (168051 windows)\n",
      "      Current totals: 2000 normal, 1000 anomaly\n",
      "      Sampling limits reached - stopping early\n",
      "   Sampling complete - sufficient data collected\n",
      "\n",
      "✅ Successfully loaded and separated windowed data!\n",
      "Normal operation windows (class 0): 2000\n",
      "Anomaly windows (classes [3, 4, 8]): 1000\n",
      "Loading time: 11.140 seconds\n",
      "Files processed: 1\n",
      "\n",
      "Processing sample windows... ✅\n",
      "\n",
      "Sample Normal Window (Window #1):\n",
      "   • Shape: (300, 4)\n",
      "   • Class: 0 (Normal Operation)\n",
      "   • Features: ['P-PDG_scaled', 'P-TPT_scaled', 'T-TPT_scaled', 'class']\n",
      "\n",
      "Normal Operation Data (Class 0):\n",
      "   • Total windows: 2000\n",
      "   • Usage: Autoencoder training\n",
      "\n",
      "Anomaly Data (Selected Classes [3, 4, 8]):\n",
      "   • Class 3: 1000 windows\n",
      "   • Total anomaly windows: 1000\n",
      "   • Usage: Anomaly detection testing\n",
      "\n",
      "Performance Summary:\n",
      "   • Total execution time: 14.475 seconds\n",
      "   • Data loading time: 11.140 seconds\n",
      "   • File format: pickle\n",
      "   • Folds processed: 1\n",
      "\n",
      "Dataset Summary for Novelty Detection:\n",
      "   • Normal training data: 2000 windows\n",
      "   • Anomaly test data: 1000 windows\n",
      "   • Window dimensions: (300, 4)\n",
      "   • Selected anomaly classes: [np.str_('3')]\n",
      "\n",
      "Configuration Notes:\n",
      "   • Sampling enabled for faster processing\n",
      "   • Only classes [3, 4, 8] used as anomalies\n",
      "   • To use full dataset: Set ENABLE_SAMPLING = False\n",
      "   • To use all folds: Set USE_SINGLE_FOLD = False\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# OPTIMIZED LOADING: 3W DATASET FOR UNSUPERVISED NOVELTY DETECTION\n",
    "# ============================================================\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Loading 3W Dataset for Autoencoder Novelty Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import data loading utilities\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "print(\"Importing modules...\", end=\" \")\n",
    "from src.data_persistence import DataPersistence\n",
    "from src import config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"✅\")\n",
    "\n",
    "# ============================================================\n",
    "# OPTIMIZATION SETTINGS - ADJUST FOR SPEED vs COMPLETENESS\n",
    "# ============================================================\n",
    "USE_SINGLE_FOLD = (\n",
    "    True  # True: Fast loading (one fold), False: Complete dataset (all folds)\n",
    ")\n",
    "TARGET_FOLD = \"fold_1\"  # Which fold to use for single fold loading\n",
    "MAX_NORMAL_SAMPLES = 2000  # Limit normal samples for faster processing\n",
    "MAX_ANOMALY_SAMPLES = 1000  # Limit anomaly samples for faster processing\n",
    "ENABLE_SAMPLING = True  # True: Apply sampling limits, False: Load all available data\n",
    "\n",
    "# ============================================================\n",
    "# ANOMALY CLASS CONFIGURATION: ONLY CLASSES 3, 4, 8\n",
    "# ============================================================\n",
    "SELECTED_ANOMALY_CLASSES = [3, 4, 8]  # Only use these classes as anomalies\n",
    "print(f\"Anomaly Class Configuration: {SELECTED_ANOMALY_CLASSES}\")\n",
    "\n",
    "print(f\"Optimization Settings:\")\n",
    "print(f\"   • Single fold loading: {USE_SINGLE_FOLD}\")\n",
    "if USE_SINGLE_FOLD:\n",
    "    print(f\"   • Target fold: {TARGET_FOLD}\")\n",
    "print(f\"   • Sampling enabled: {ENABLE_SAMPLING}\")\n",
    "if ENABLE_SAMPLING:\n",
    "    print(f\"   • Max normal samples: {MAX_NORMAL_SAMPLES}\")\n",
    "    print(f\"   • Max anomaly samples: {MAX_ANOMALY_SAMPLES}\")\n",
    "print(f\"   • Selected anomaly classes: {SELECTED_ANOMALY_CLASSES}\")\n",
    "\n",
    "try:\n",
    "    print(f\"\\nInitializing data persistence...\", end=\" \")\n",
    "    persistence = DataPersistence(base_dir=config.PROCESSED_DATA_DIR, verbose=False)\n",
    "    print(\"✅\")\n",
    "\n",
    "    print(f\"Using format: {config.SAVE_FORMAT}\")\n",
    "\n",
    "    # Check if windowed directory exists\n",
    "    windowed_dir = os.path.join(persistence.cv_splits_dir, \"windowed\")\n",
    "    print(f\"Checking windowed directory: {windowed_dir}...\", end=\" \")\n",
    "\n",
    "    if not os.path.exists(windowed_dir):\n",
    "        print(\"❌\")\n",
    "        print(\n",
    "            \"❌ No windowed data directory found. Please run Data Treatment notebook first to generate windowed time series data.\"\n",
    "        )\n",
    "        raise FileNotFoundError(\"Windowed data directory not found\")\n",
    "    else:\n",
    "        print(\"✅\")\n",
    "\n",
    "        # Look for fold directories\n",
    "        print(\"Looking for fold directories...\", end=\" \")\n",
    "        fold_dirs = [\n",
    "            d\n",
    "            for d in os.listdir(windowed_dir)\n",
    "            if d.startswith(\"fold_\") and os.path.isdir(os.path.join(windowed_dir, d))\n",
    "        ]\n",
    "        fold_dirs.sort()\n",
    "        print(f\"✅ Found {len(fold_dirs)} folds: {fold_dirs}\")\n",
    "\n",
    "        if not fold_dirs:\n",
    "            print(\"❌ No fold directories found in windowed data.\")\n",
    "            raise FileNotFoundError(\"No fold directories found\")\n",
    "\n",
    "        # Determine which folds to process\n",
    "        if USE_SINGLE_FOLD:\n",
    "            if TARGET_FOLD in fold_dirs:\n",
    "                process_folds = [TARGET_FOLD]\n",
    "                print(f\"Using single fold: {TARGET_FOLD}\")\n",
    "            else:\n",
    "                process_folds = [fold_dirs[0]]  # Use first available fold\n",
    "                print(\n",
    "                    f\"⚠️ Target fold '{TARGET_FOLD}' not found, using: {process_folds[0]}\"\n",
    "                )\n",
    "        else:\n",
    "            process_folds = fold_dirs\n",
    "            print(f\"Using all {len(fold_dirs)} folds\")\n",
    "\n",
    "        # Separate data containers for normal (class 0) and anomaly (selected classes only) data\n",
    "        normal_windows = []  # Class 0 for autoencoder training\n",
    "        normal_classes = []\n",
    "        anomaly_windows = []  # Only selected classes for anomaly testing\n",
    "        anomaly_classes = []\n",
    "\n",
    "        load_start = time.time()\n",
    "        total_files_processed = 0\n",
    "\n",
    "        for fold_idx, fold_name in enumerate(process_folds):\n",
    "            fold_path = os.path.join(windowed_dir, fold_name)\n",
    "\n",
    "            print(\n",
    "                f\"\\nProcessing {fold_name} ({fold_idx + 1}/{len(process_folds)})...\"\n",
    "            )\n",
    "\n",
    "            # Process training and test data\n",
    "            for data_type in [\"train\", \"test\"]:\n",
    "                print(f\"   Loading {data_type} data...\", end=\" \")\n",
    "\n",
    "                # Try pickle first, then parquet\n",
    "                pickle_file = os.path.join(\n",
    "                    fold_path, f\"{data_type}_windowed.{config.SAVE_FORMAT}\"\n",
    "                )\n",
    "                parquet_file = os.path.join(fold_path, f\"{data_type}_windowed.parquet\")\n",
    "\n",
    "                fold_dfs, fold_classes = [], []\n",
    "\n",
    "                if os.path.exists(pickle_file):\n",
    "                    try:\n",
    "                        fold_dfs, fold_classes = persistence._load_dataframes(\n",
    "                            pickle_file, config.SAVE_FORMAT\n",
    "                        )\n",
    "                        print(f\"✅ ({len(fold_dfs)} windows)\")\n",
    "                        total_files_processed += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Pickle error: {str(e)}\")\n",
    "                        # Try parquet as fallback\n",
    "                        if os.path.exists(parquet_file):\n",
    "                            try:\n",
    "                                fold_dfs, fold_classes = persistence._load_from_parquet(\n",
    "                                    parquet_file\n",
    "                                )\n",
    "                                print(f\"✅ Parquet fallback ({len(fold_dfs)} windows)\")\n",
    "                                total_files_processed += 1\n",
    "                            except Exception as e2:\n",
    "                                print(f\"❌ Parquet fallback failed: {str(e2)}\")\n",
    "\n",
    "                elif os.path.exists(parquet_file):\n",
    "                    try:\n",
    "                        fold_dfs, fold_classes = persistence._load_from_parquet(\n",
    "                            parquet_file\n",
    "                        )\n",
    "                        print(f\"✅ ({len(fold_dfs)} windows)\")\n",
    "                        total_files_processed += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Parquet error: {str(e)}\")\n",
    "                else:\n",
    "                    print(\"❌ No data file found\")\n",
    "\n",
    "                # Separate normal (class 0) from selected anomaly classes only\n",
    "                for df, cls in zip(fold_dfs, fold_classes):\n",
    "                    if str(cls) == \"0\":  # Normal operation\n",
    "                        if (\n",
    "                            not ENABLE_SAMPLING\n",
    "                            or len(normal_windows) < MAX_NORMAL_SAMPLES\n",
    "                        ):\n",
    "                            normal_windows.append(df)\n",
    "                            normal_classes.append(cls)\n",
    "                    elif int(cls) in SELECTED_ANOMALY_CLASSES:  # Only selected fault classes\n",
    "                        if (\n",
    "                            not ENABLE_SAMPLING\n",
    "                            or len(anomaly_windows) < MAX_ANOMALY_SAMPLES\n",
    "                        ):\n",
    "                            anomaly_windows.append(df)\n",
    "                            anomaly_classes.append(cls)\n",
    "\n",
    "                # Show progress\n",
    "                if ENABLE_SAMPLING:\n",
    "                    print(\n",
    "                        f\"      Current totals: {len(normal_windows)} normal, {len(anomaly_windows)} anomaly\"\n",
    "                    )\n",
    "                    if (\n",
    "                        len(normal_windows) >= MAX_NORMAL_SAMPLES\n",
    "                        and len(anomaly_windows) >= MAX_ANOMALY_SAMPLES\n",
    "                    ):\n",
    "                        print(f\"      Sampling limits reached - stopping early\")\n",
    "                        break\n",
    "\n",
    "            # Early exit if sampling limits reached\n",
    "            if (\n",
    "                ENABLE_SAMPLING\n",
    "                and len(normal_windows) >= MAX_NORMAL_SAMPLES\n",
    "                and len(anomaly_windows) >= MAX_ANOMALY_SAMPLES\n",
    "            ):\n",
    "                print(f\"   Sampling complete - sufficient data collected\")\n",
    "                break\n",
    "\n",
    "        load_time = time.time() - load_start\n",
    "\n",
    "        if normal_windows and anomaly_windows:\n",
    "            print(f\"\\n✅ Successfully loaded and separated windowed data!\")\n",
    "            print(f\"Normal operation windows (class 0): {len(normal_windows)}\")\n",
    "            print(f\"Anomaly windows (classes {SELECTED_ANOMALY_CLASSES}): {len(anomaly_windows)}\")\n",
    "            print(f\"Loading time: {load_time:.3f} seconds\")\n",
    "            print(f\"Files processed: {total_files_processed}\")\n",
    "\n",
    "            # Display sample window information\n",
    "            if normal_windows:\n",
    "                print(\"\\nProcessing sample windows...\", end=\" \")\n",
    "                first_normal_window = normal_windows[0]\n",
    "                print(\"✅\")\n",
    "\n",
    "                print(f\"\\nSample Normal Window (Window #1):\")\n",
    "                print(f\"   • Shape: {first_normal_window.shape}\")\n",
    "                print(f\"   • Class: {normal_classes[0]} (Normal Operation)\")\n",
    "                print(f\"   • Features: {list(first_normal_window.columns)}\")\n",
    "\n",
    "                # Show class distribution\n",
    "                print(f\"\\nNormal Operation Data (Class 0):\")\n",
    "                print(f\"   • Total windows: {len(normal_windows)}\")\n",
    "                print(f\"   • Usage: Autoencoder training\")\n",
    "\n",
    "                print(f\"\\nAnomaly Data (Selected Classes {SELECTED_ANOMALY_CLASSES}):\")\n",
    "                anomaly_unique, anomaly_counts = np.unique(\n",
    "                    anomaly_classes, return_counts=True\n",
    "                )\n",
    "                for cls, count in zip(anomaly_unique, anomaly_counts):\n",
    "                    print(f\"   • Class {cls}: {count} windows\")\n",
    "                print(f\"   • Total anomaly windows: {len(anomaly_windows)}\")\n",
    "                print(f\"   • Usage: Anomaly detection testing\")\n",
    "\n",
    "                total_time = time.time() - start_time\n",
    "                print(f\"\\nPerformance Summary:\")\n",
    "                print(f\"   • Total execution time: {total_time:.3f} seconds\")\n",
    "                print(f\"   • Data loading time: {load_time:.3f} seconds\")\n",
    "                print(f\"   • File format: {config.SAVE_FORMAT}\")\n",
    "                print(f\"   • Folds processed: {len(process_folds)}\")\n",
    "\n",
    "                print(f\"\\nDataset Summary for Novelty Detection:\")\n",
    "                print(f\"   • Normal training data: {len(normal_windows)} windows\")\n",
    "                print(f\"   • Anomaly test data: {len(anomaly_windows)} windows\")\n",
    "                print(f\"   • Window dimensions: {first_normal_window.shape}\")\n",
    "                print(f\"   • Selected anomaly classes: {sorted(anomaly_unique)}\")\n",
    "\n",
    "                if ENABLE_SAMPLING:\n",
    "                    print(f\"\\nConfiguration Notes:\")\n",
    "                    print(f\"   • Sampling enabled for faster processing\")\n",
    "                    print(f\"   • Only classes {SELECTED_ANOMALY_CLASSES} used as anomalies\")\n",
    "                    print(f\"   • To use full dataset: Set ENABLE_SAMPLING = False\")\n",
    "                    print(f\"   • To use all folds: Set USE_SINGLE_FOLD = False\")\n",
    "\n",
    "            else:\n",
    "                print(\"⚠️ No normal operation windows found\")\n",
    "\n",
    "        else:\n",
    "            print(\"⚠️ Insufficient data found for novelty detection\")\n",
    "            print(f\"   • Normal windows: {len(normal_windows)}\")\n",
    "            print(f\"   • Anomaly windows: {len(anomaly_windows)}\")\n",
    "            normal_windows = []\n",
    "            normal_classes = []\n",
    "            anomaly_windows = []\n",
    "            anomaly_classes = []\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error loading data: {str(e)}\")\n",
    "    print(f\"\\nTroubleshooting:\")\n",
    "    print(f\"   1. Make sure 'Data Treatment.ipynb' ran completely\")\n",
    "    print(f\"   2. Check if windowed data was saved successfully\")\n",
    "    print(f\"   3. Verify the processed_data directory exists\")\n",
    "    print(f\"   4. Check if pickle files are corrupted\")\n",
    "    print(f\"   5. Try using parquet format instead\")\n",
    "\n",
    "    # Show detailed error information\n",
    "    import traceback\n",
    "\n",
    "    print(f\"\\nDetailed error information:\")\n",
    "    print(traceback.format_exc())\n",
    "\n",
    "    # Initialize empty variables for error case\n",
    "    normal_windows = []\n",
    "    normal_classes = []\n",
    "    anomaly_windows = []\n",
    "    anomaly_classes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229fa361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Autoencoder for Anomaly Detection\n",
      "=============================================\n",
      "✅ Data available for processing\n",
      "Normal operation windows: 2000\n",
      "Anomaly windows: 1000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# IMPORT REQUIRED LIBRARIES AND MODULES\n",
    "# ============================================================\n",
    "\n",
    "print(\"LSTM Autoencoder for Anomaly Detection\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Import our custom modules\n",
    "from src.autoencoder_models import StableLSTMAutoencoder\n",
    "from src.unsupervised_preprocessing import UnsupervisedDataPreprocessor\n",
    "from src.anomaly_detection import AnomalyDetector, visualize_latent_space\n",
    "\n",
    "# Set TensorFlow to avoid NaN issues\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "# Check if we have loaded data from previous cell\n",
    "if (\n",
    "    \"normal_windows\" in locals()\n",
    "    and normal_windows is not None\n",
    "    and len(normal_windows) > 0\n",
    "    and \"anomaly_windows\" in locals()\n",
    "    and anomaly_windows is not None\n",
    "    and len(anomaly_windows) > 0\n",
    "):\n",
    "\n",
    "    print(\"✅ Data available for processing\")\n",
    "    print(f\"Normal operation windows: {len(normal_windows)}\")\n",
    "    print(f\"Anomaly windows: {len(anomaly_windows)}\")\n",
    "    data_ready = True\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"❌ No data available. Please run the previous cell first to load normal and anomaly data.\"\n",
    "    )\n",
    "    data_ready = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647600ad",
   "metadata": {},
   "source": [
    "## 🧩 Modular LSTM Autoencoder Implementation\n",
    "\n",
    "This section implements a **stable LSTM autoencoder** for anomaly detection using a **modular approach**. The code has been organized into reusable components in the `src/` directory:\n",
    "\n",
    "### 📦 New Modules Created:\n",
    "\n",
    "1. **`autoencoder_models.py`** - Contains the `StableLSTMAutoencoder` class with:\n",
    "   - Stable architecture with gradient clipping\n",
    "   - Conservative hyperparameters for numerical stability\n",
    "   - Built-in training and prediction methods\n",
    "\n",
    "2. **`unsupervised_preprocessing.py`** - Contains the `UnsupervisedDataPreprocessor` class with:\n",
    "   - Smart data sampling for training efficiency\n",
    "   - Robust data validation and conversion\n",
    "   - Numerical stability enhancements\n",
    "\n",
    "3. **`anomaly_detection.py`** - Contains the `AnomalyDetector` class with:\n",
    "   - Reconstruction error computation\n",
    "   - Threshold determination methods\n",
    "   - Performance evaluation and visualization\n",
    "\n",
    "### 🎯 Benefits of This Approach:\n",
    "\n",
    "- **Modularity**: Each component has a single responsibility\n",
    "- **Reusability**: Classes can be used in other projects\n",
    "- **Maintainability**: Easier to debug and modify individual components\n",
    "- **Stability**: Enhanced numerical safeguards and error handling\n",
    "- **Clarity**: Notebook cells are focused and easier to understand\n",
    "\n",
    "### 🚀 Usage Pattern:\n",
    "\n",
    "The following cells demonstrate the complete pipeline from data preprocessing through model training to anomaly detection evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c78e6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessing Pipeline\n",
      "===================================\n",
      "🔧 Complete Data Preparation Pipeline\n",
      "==================================================\n",
      "⚡ Smart Data Sampling for Training Efficiency\n",
      "==================================================\n",
      "🎯 Training optimization settings:\n",
      "   • Max normal samples for training: 1000\n",
      "   • Max anomaly samples for testing: 300\n",
      "📊 Sampling 1000 normal windows from 2000 available...\n",
      "📊 Sampling 300 anomaly windows from 1000 available...\n",
      "📊 Converting normal windows to arrays... ✅ (1000 valid processed)\n",
      "📊 Converting anomaly windows to arrays... ✅ (300 valid processed)\n",
      "🔍 Validating array shapes and data quality...\n",
      "Expected shape: (300, 3)\n",
      "✅ Valid arrays: 1000 normal, 300 anomaly\n",
      "📊 Data quality checks:\n",
      "   • Normal data range: [0.000, 1.000]\n",
      "   • Anomaly data range: [0.000, 1.000]\n",
      "   • Normal data finite: True\n",
      "   • Anomaly data finite: True\n",
      "📊 Additional Data Normalization for Stability\n",
      "=============================================\n",
      "📏 Enhanced data characteristics:\n",
      "   • Normal data range: [0.001, 0.999]\n",
      "   • Anomaly data range: [0.001, 0.999]\n",
      "   • Data clipped to avoid extreme values\n",
      "   • Float32 precision for stability\n",
      "\n",
      "📐 Final Data Shapes:\n",
      "   • Normal data: (1000, 300, 3)\n",
      "   • Anomaly data: (300, 300, 3)\n",
      "   • Time steps per window: 300\n",
      "   • Features per time step: 3 (class column removed)\n",
      "\n",
      "✅ Data preprocessing completed successfully!\n",
      "Final Configuration:\n",
      "   • Time steps per window: 300\n",
      "   • Features per time step: 3 (class column removed)\n",
      "   • Normal training samples: 1000\n",
      "   • Anomaly test samples: 300\n",
      "📊 Converting normal windows to arrays... ✅ (1000 valid processed)\n",
      "📊 Converting anomaly windows to arrays... ✅ (300 valid processed)\n",
      "🔍 Validating array shapes and data quality...\n",
      "Expected shape: (300, 3)\n",
      "✅ Valid arrays: 1000 normal, 300 anomaly\n",
      "📊 Data quality checks:\n",
      "   • Normal data range: [0.000, 1.000]\n",
      "   • Anomaly data range: [0.000, 1.000]\n",
      "   • Normal data finite: True\n",
      "   • Anomaly data finite: True\n",
      "📊 Additional Data Normalization for Stability\n",
      "=============================================\n",
      "📏 Enhanced data characteristics:\n",
      "   • Normal data range: [0.001, 0.999]\n",
      "   • Anomaly data range: [0.001, 0.999]\n",
      "   • Data clipped to avoid extreme values\n",
      "   • Float32 precision for stability\n",
      "\n",
      "📐 Final Data Shapes:\n",
      "   • Normal data: (1000, 300, 3)\n",
      "   • Anomaly data: (300, 300, 3)\n",
      "   • Time steps per window: 300\n",
      "   • Features per time step: 3 (class column removed)\n",
      "\n",
      "✅ Data preprocessing completed successfully!\n",
      "Final Configuration:\n",
      "   • Time steps per window: 300\n",
      "   • Features per time step: 3 (class column removed)\n",
      "   • Normal training samples: 1000\n",
      "   • Anomaly test samples: 300\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DATA PREPROCESSING AND SAMPLING\n",
    "# ============================================================\n",
    "\n",
    "if data_ready:\n",
    "    print(\"Data Preprocessing Pipeline\")\n",
    "    print(\"=\" * 35)\n",
    "\n",
    "    # Initialize data preprocessor with conservative settings for stability\n",
    "    preprocessor = UnsupervisedDataPreprocessor(\n",
    "        max_training_samples=1000,  # Reduced for stability\n",
    "        max_anomaly_samples=300,  # Reduced for stability\n",
    "        random_seed=42,\n",
    "    )\n",
    "\n",
    "    # Run the complete preprocessing pipeline\n",
    "    normal_scaled, anomaly_scaled, data_info = preprocessor.prepare_full_pipeline(\n",
    "        normal_windows, anomaly_windows\n",
    "    )\n",
    "\n",
    "    print(f\"\\n✅ Data preprocessing completed successfully!\")\n",
    "    print(f\"Final Configuration:\")\n",
    "    print(f\"   • Time steps per window: {data_info['time_steps']}\")\n",
    "    print(\n",
    "        f\"   • Features per time step: {data_info['n_features']} (class column removed)\"\n",
    "    )\n",
    "    print(f\"   • Normal training samples: {data_info['n_normal_samples']}\")\n",
    "    print(f\"   • Anomaly test samples: {data_info['n_anomaly_samples']}\")\n",
    "\n",
    "    preprocessing_complete = True\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot proceed with preprocessing - no data available\")\n",
    "    preprocessing_complete = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee4b074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and Training LSTM Autoencoder\n",
      "=============================================\n",
      "🏗️ Building Stable LSTM Autoencoder:\n",
      "   • Input shape: (300, 3)\n",
      "   • Encoder LSTM units: 32\n",
      "   • Latent dimension: 16\n",
      "   • Decoder LSTM units: 32\n",
      "WARNING:tensorflow:From c:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "✅ Stable LSTM Autoencoder model created\n",
      "   • Total parameters: 11,507\n",
      "   • Gradient clipping enabled (clipnorm=1.0)\n",
      "   • Conservative learning rate (0.0005)\n",
      "\n",
      "Training Data Split:\n",
      "   • Training samples: 800\n",
      "   • Validation samples: 200\n",
      "\n",
      "Starting Training...\n",
      "🚂 Training LSTM Autoencoder:\n",
      "   • Training samples: 800\n",
      "   • Validation samples: 200\n",
      "   • Max epochs: 30\n",
      "   • Batch size: 32\n",
      "Epoch 1/30\n",
      "✅ Stable LSTM Autoencoder model created\n",
      "   • Total parameters: 11,507\n",
      "   • Gradient clipping enabled (clipnorm=1.0)\n",
      "   • Conservative learning rate (0.0005)\n",
      "\n",
      "Training Data Split:\n",
      "   • Training samples: 800\n",
      "   • Validation samples: 200\n",
      "\n",
      "Starting Training...\n",
      "🚂 Training LSTM Autoencoder:\n",
      "   • Training samples: 800\n",
      "   • Validation samples: 200\n",
      "   • Max epochs: 30\n",
      "   • Batch size: 32\n",
      "Epoch 1/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 199ms/step - loss: 0.0992 - mae: 0.2734 - val_loss: 0.0641 - val_mae: 0.2200 - learning_rate: 5.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 199ms/step - loss: 0.0992 - mae: 0.2734 - val_loss: 0.0641 - val_mae: 0.2200 - learning_rate: 5.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - loss: 0.0564 - mae: 0.1968 - val_loss: 0.0404 - val_mae: 0.1585 - learning_rate: 5.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - loss: 0.0564 - mae: 0.1968 - val_loss: 0.0404 - val_mae: 0.1585 - learning_rate: 5.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - loss: 0.0430 - mae: 0.1636 - val_loss: 0.0262 - val_mae: 0.1273 - learning_rate: 5.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - loss: 0.0430 - mae: 0.1636 - val_loss: 0.0262 - val_mae: 0.1273 - learning_rate: 5.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - loss: 0.0378 - mae: 0.1508 - val_loss: 0.0208 - val_mae: 0.1114 - learning_rate: 5.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - loss: 0.0378 - mae: 0.1508 - val_loss: 0.0208 - val_mae: 0.1114 - learning_rate: 5.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 0.0320 - mae: 0.1358 - val_loss: 0.0150 - val_mae: 0.0910 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 0.0320 - mae: 0.1358 - val_loss: 0.0150 - val_mae: 0.0910 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0293 - mae: 0.1262 - val_loss: 0.0121 - val_mae: 0.0790 - learning_rate: 5.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0293 - mae: 0.1262 - val_loss: 0.0121 - val_mae: 0.0790 - learning_rate: 5.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 0.0274 - mae: 0.1211 - val_loss: 0.0114 - val_mae: 0.0752 - learning_rate: 5.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 0.0274 - mae: 0.1211 - val_loss: 0.0114 - val_mae: 0.0752 - learning_rate: 5.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0258 - mae: 0.1167 - val_loss: 0.0106 - val_mae: 0.0711 - learning_rate: 5.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0258 - mae: 0.1167 - val_loss: 0.0106 - val_mae: 0.0711 - learning_rate: 5.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - loss: 0.0234 - mae: 0.1101 - val_loss: 0.0103 - val_mae: 0.0695 - learning_rate: 5.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - loss: 0.0234 - mae: 0.1101 - val_loss: 0.0103 - val_mae: 0.0695 - learning_rate: 5.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 0.0248 - mae: 0.1110 - val_loss: 0.0099 - val_mae: 0.0675 - learning_rate: 5.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 0.0248 - mae: 0.1110 - val_loss: 0.0099 - val_mae: 0.0675 - learning_rate: 5.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 0.0241 - mae: 0.1090 - val_loss: 0.0105 - val_mae: 0.0724 - learning_rate: 5.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 0.0241 - mae: 0.1090 - val_loss: 0.0105 - val_mae: 0.0724 - learning_rate: 5.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0223 - mae: 0.1045 - val_loss: 0.0097 - val_mae: 0.0667 - learning_rate: 5.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0223 - mae: 0.1045 - val_loss: 0.0097 - val_mae: 0.0667 - learning_rate: 5.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 0.0214 - mae: 0.1017 - val_loss: 0.0098 - val_mae: 0.0703 - learning_rate: 5.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 0.0214 - mae: 0.1017 - val_loss: 0.0098 - val_mae: 0.0703 - learning_rate: 5.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 0.0228 - mae: 0.1024 - val_loss: 0.0099 - val_mae: 0.0670 - learning_rate: 5.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 0.0228 - mae: 0.1024 - val_loss: 0.0099 - val_mae: 0.0670 - learning_rate: 5.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0220 - mae: 0.1005 - val_loss: 0.0093 - val_mae: 0.0626 - learning_rate: 5.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0220 - mae: 0.1005 - val_loss: 0.0093 - val_mae: 0.0626 - learning_rate: 5.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - loss: 0.0217 - mae: 0.0998 - val_loss: 0.0099 - val_mae: 0.0693 - learning_rate: 5.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - loss: 0.0217 - mae: 0.0998 - val_loss: 0.0099 - val_mae: 0.0693 - learning_rate: 5.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - loss: 0.0204 - mae: 0.0967 - val_loss: 0.0094 - val_mae: 0.0649 - learning_rate: 5.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - loss: 0.0204 - mae: 0.0967 - val_loss: 0.0094 - val_mae: 0.0649 - learning_rate: 5.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 0.0203 - mae: 0.0967 - val_loss: 0.0094 - val_mae: 0.0649 - learning_rate: 5.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 0.0203 - mae: 0.0967 - val_loss: 0.0094 - val_mae: 0.0649 - learning_rate: 5.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0201 - mae: 0.0948 - val_loss: 0.0087 - val_mae: 0.0603 - learning_rate: 5.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0201 - mae: 0.0948 - val_loss: 0.0087 - val_mae: 0.0603 - learning_rate: 5.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0192 - mae: 0.0935 - val_loss: 0.0096 - val_mae: 0.0678 - learning_rate: 5.0000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0192 - mae: 0.0935 - val_loss: 0.0096 - val_mae: 0.0678 - learning_rate: 5.0000e-04\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting Training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m training_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 33\u001b[0m training_success \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_normal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_normal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Conservative for stability\u001b[39;49;00m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Conservative for stability\u001b[39;49;00m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m training_start\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\Documents\\GitHub\\3W\\resources\\introduction_to_ml_applied_to_mts\\src\\autoencoder_models.py:160\u001b[0m, in \u001b[0;36mStableLSTMAutoencoder.train\u001b[1;34m(self, train_data, val_data, epochs, batch_size, verbose)\u001b[0m\n\u001b[0;32m    155\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(\n\u001b[0;32m    156\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    157\u001b[0m )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# Train autoencoder\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Autoencoder: input = target\u001b[39;49;00m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Check if training was successful\u001b[39;00m\n\u001b[0;32m    172\u001b[0m final_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    376\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[1;32m--> 377\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    218\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    219\u001b[0m     ):\n\u001b[1;32m--> 220\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BUILD AND TRAIN LSTM AUTOENCODER\n",
    "# ============================================================\n",
    "\n",
    "if preprocessing_complete:\n",
    "    print(\"Building and Training LSTM Autoencoder\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    # Initialize the autoencoder with conservative parameters for stability\n",
    "    autoencoder = StableLSTMAutoencoder(\n",
    "        time_steps=data_info[\"time_steps\"],\n",
    "        n_features=data_info[\"n_features\"],\n",
    "        latent_dim=16,  # Conservative for stability\n",
    "        lstm_units=32,  # Conservative for stability\n",
    "    )\n",
    "\n",
    "    # Build the model\n",
    "    model = autoencoder.build_model()\n",
    "\n",
    "    # Split normal data into train/validation (80/20 split)\n",
    "    split_idx = int(0.8 * len(normal_scaled))\n",
    "    train_normal = normal_scaled[:split_idx]\n",
    "    val_normal = normal_scaled[split_idx:]\n",
    "\n",
    "    print(f\"\\nTraining Data Split:\")\n",
    "    print(f\"   • Training samples: {len(train_normal)}\")\n",
    "    print(f\"   • Validation samples: {len(val_normal)}\")\n",
    "\n",
    "    # Train the autoencoder\n",
    "    print(f\"\\nStarting Training...\")\n",
    "    training_start = time.time()\n",
    "\n",
    "    training_success = autoencoder.train(\n",
    "        train_data=train_normal,\n",
    "        val_data=val_normal,\n",
    "        epochs=30,  # Conservative for stability\n",
    "        batch_size=32,  # Conservative for stability\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    training_time = time.time() - training_start\n",
    "    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "\n",
    "    if training_success:\n",
    "        print(\"✅ Model trained successfully\")\n",
    "        model_ready = True\n",
    "    else:\n",
    "        print(\"❌ Training failed - check for numerical stability issues\")\n",
    "        model_ready = False\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot build model - preprocessing not completed\")\n",
    "    model_ready = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1072a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "if model_ready and autoencoder.history is not None:\n",
    "    print(\"📊 Training History Visualization\")\n",
    "    print(\"=\" * 35)\n",
    "\n",
    "    history = autoencoder.history.history\n",
    "\n",
    "    # Create training plots\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history[\"loss\"], label=\"Training Loss\", linewidth=2, color=\"blue\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Validation Loss\", linewidth=2, color=\"orange\")\n",
    "    plt.title(\"Model Loss During Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # MAE plot\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history[\"mae\"], label=\"Training MAE\", linewidth=2, color=\"blue\")\n",
    "    plt.plot(history[\"val_mae\"], label=\"Validation MAE\", linewidth=2, color=\"orange\")\n",
    "    plt.title(\"Model MAE During Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Absolute Error\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Learning rate plot (if available)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    if \"lr\" in history:\n",
    "        plt.plot(history[\"lr\"], label=\"Learning Rate\", linewidth=2, color=\"green\")\n",
    "        plt.title(\"Learning Rate Schedule\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Learning Rate\")\n",
    "        plt.yscale(\"log\")\n",
    "    else:\n",
    "        plt.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"Learning Rate\\nHistory\\nNot Available\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=plt.gca().transAxes,\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"lightgray\"),\n",
    "        )\n",
    "        plt.title(\"Learning Rate Schedule\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print training summary\n",
    "    final_train_loss = history[\"loss\"][-1]\n",
    "    final_val_loss = history[\"val_loss\"][-1]\n",
    "    epochs_trained = len(history[\"loss\"])\n",
    "\n",
    "    print(f\"\\n📋 Training Summary:\")\n",
    "    print(f\"   • Epochs trained: {epochs_trained}\")\n",
    "    print(f\"   • Final training loss: {final_train_loss:.6f}\")\n",
    "    print(f\"   • Final validation loss: {final_val_loss:.6f}\")\n",
    "    print(f\"   • Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "    if final_val_loss < final_train_loss * 1.5:\n",
    "        print(\"   ✅ No significant overfitting detected\")\n",
    "    else:\n",
    "        print(\"   ⚠️ Possible overfitting - validation loss higher than expected\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No training history available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f47835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ANOMALY DETECTION AND EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "if model_ready:\n",
    "    print(\"🔍 Anomaly Detection and Evaluation\")\n",
    "    print(\"=\" * 35)\n",
    "\n",
    "    # Initialize anomaly detector\n",
    "    detector = AnomalyDetector()\n",
    "\n",
    "    # Compute reconstruction errors\n",
    "    normal_errors, anomaly_errors = detector.compute_reconstruction_errors(\n",
    "        autoencoder=autoencoder, normal_data=normal_scaled, anomaly_data=anomaly_scaled\n",
    "    )\n",
    "\n",
    "    # Determine threshold using 95th percentile of normal errors\n",
    "    threshold = detector.determine_threshold(method=\"percentile\", percentile=95)\n",
    "\n",
    "    # Evaluate detection performance\n",
    "    metrics = detector.evaluate_detection()\n",
    "\n",
    "    print(f\"\\n🎯 Anomaly Detection Results:\")\n",
    "    print(f\"   • Threshold: {threshold:.6f}\")\n",
    "    print(f\"   • Normal accuracy: {metrics['normal_accuracy']:.3f}\")\n",
    "    print(f\"   • Anomaly accuracy: {metrics['anomaly_accuracy']:.3f}\")\n",
    "    print(f\"   • Overall accuracy: {metrics['overall_accuracy']:.3f}\")\n",
    "\n",
    "    detection_complete = True\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot perform anomaly detection - model not ready\")\n",
    "    detection_complete = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78676a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESULTS VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "if detection_complete:\n",
    "    print(\"📊 Comprehensive Results Visualization\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # Plot reconstruction error distributions\n",
    "    print(\"📈 Plotting reconstruction error distributions...\")\n",
    "    detector.plot_error_distributions(figsize=(15, 10))\n",
    "\n",
    "    # Plot ROC curve\n",
    "    print(\"📈 Plotting ROC curve...\")\n",
    "    roc_auc = detector.plot_roc_curve()\n",
    "\n",
    "    print(f\"\\n📊 Visualization Summary:\")\n",
    "    print(f\"   ✅ Error distribution plots generated\")\n",
    "    print(f\"   ✅ ROC curve generated (AUC: {roc_auc:.3f})\")\n",
    "    print(f\"   ✅ Comprehensive analysis completed\")\n",
    "\n",
    "    if roc_auc > 0.8:\n",
    "        print(f\"   🎉 Excellent anomaly detection performance!\")\n",
    "    elif roc_auc > 0.7:\n",
    "        print(f\"   👍 Good anomaly detection performance\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ Consider tuning model parameters for better performance\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot generate visualizations - detection not completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f25ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LATENT SPACE VISUALIZATION (OPTIONAL)\n",
    "# ============================================================\n",
    "\n",
    "if detection_complete:\n",
    "    print(\"🎨 Advanced Analysis: Latent Space Visualization\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    try:\n",
    "        # Visualize latent space with t-SNE\n",
    "        visualize_latent_space(\n",
    "            autoencoder=autoencoder,\n",
    "            normal_data=normal_scaled,\n",
    "            anomaly_data=anomaly_scaled,\n",
    "            n_samples=500,  # Sample for faster computation\n",
    "        )\n",
    "\n",
    "        print(f\"\\n✅ Latent space visualization completed\")\n",
    "        print(f\"   • This visualization shows how the autoencoder learns to\")\n",
    "        print(f\"     separate normal and anomalous patterns in its internal\")\n",
    "        print(f\"     latent representation\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Latent space visualization failed: {str(e)}\")\n",
    "        print(f\"   • This is optional and doesn't affect the main analysis\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot visualize latent space - detection not completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d414dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Reconstruction Error Analysis and Anomaly Threshold\n",
      "============================================================\n",
      "\n",
      "📊 Computing Reconstruction Errors\n",
      "========================================\n",
      "🔵 Computing reconstruction errors for normal data... ✅\n",
      "🔴 Computing reconstruction errors for anomaly data... ✅\n",
      "🔴 Computing reconstruction errors for anomaly data... ✅\n",
      "\n",
      "📈 Statistical Threshold Determination\n",
      "==========================================\n",
      "📊 Normal Data Reconstruction Error Statistics:\n",
      "   • Mean error: 0.014571\n",
      "   • Standard deviation: 0.016475\n",
      "   • Min error: 0.000585\n",
      "   • Max error: 0.194340\n",
      "\n",
      "🎯 Anomaly Detection Threshold:\n",
      "   • Threshold (μ + 3σ): 0.047522\n",
      "   • Confidence level: 99.7%\n",
      "   • Normal samples above threshold: 49 / 1000\n",
      "   • Normal false positive rate: 4.90%\n",
      "\n",
      "🔴 Anomaly Detection Performance:\n",
      "   • Anomaly samples above threshold: 125 / 300\n",
      "   • Anomaly detection rate: 41.67%\n",
      "\n",
      "📊 Anomaly Detection by Class:\n",
      "✅\n",
      "\n",
      "📈 Statistical Threshold Determination\n",
      "==========================================\n",
      "📊 Normal Data Reconstruction Error Statistics:\n",
      "   • Mean error: 0.014571\n",
      "   • Standard deviation: 0.016475\n",
      "   • Min error: 0.000585\n",
      "   • Max error: 0.194340\n",
      "\n",
      "🎯 Anomaly Detection Threshold:\n",
      "   • Threshold (μ + 3σ): 0.047522\n",
      "   • Confidence level: 99.7%\n",
      "   • Normal samples above threshold: 49 / 1000\n",
      "   • Normal false positive rate: 4.90%\n",
      "\n",
      "🔴 Anomaly Detection Performance:\n",
      "   • Anomaly samples above threshold: 125 / 300\n",
      "   • Anomaly detection rate: 41.67%\n",
      "\n",
      "📊 Anomaly Detection by Class:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along axis 0; size of axis is 300 but size of corresponding boolean axis is 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m unique_anomaly_classes:\n\u001b[32m     68\u001b[39m     cls_mask = np.array(anomaly_classes) == \u001b[38;5;28mcls\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     cls_errors = \u001b[43manomaly_errors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcls_mask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     70\u001b[39m     cls_detected = np.sum(cls_errors > threshold)\n\u001b[32m     71\u001b[39m     cls_total = \u001b[38;5;28mlen\u001b[39m(cls_errors)\n",
      "\u001b[31mIndexError\u001b[39m: boolean index did not match indexed array along axis 0; size of axis is 300 but size of corresponding boolean axis is 1000"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SUMMARY AND NEXT STEPS\n",
    "# ============================================================\n",
    "\n",
    "if detection_complete:\n",
    "    print(\" Unsupervised Learning Summary\")\n",
    "    print(\"=\" * 35)\n",
    "\n",
    "    print(f\"✅ Complete LSTM Autoencoder Pipeline Executed:\")\n",
    "    print(\n",
    "        f\"   • Data preprocessing: {data_info['n_normal_samples']} normal, {data_info['n_anomaly_samples']} anomaly samples\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   • Model architecture: {autoencoder.lstm_units} LSTM units, {autoencoder.latent_dim} latent dimensions\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   • Training: {len(autoencoder.history.history['loss'])} epochs, final loss: {autoencoder.history.history['loss'][-1]:.6f}\"\n",
    "    )\n",
    "    print(f\"   • Anomaly detection: {metrics['overall_accuracy']:.3f} overall accuracy\")\n",
    "    print(f\"   • ROC AUC: {roc_auc:.3f}\")\n",
    "\n",
    "    print(f\"\\n🎓 Key Learning Outcomes:\")\n",
    "    print(f\"   • LSTM autoencoders can learn normal operation patterns\")\n",
    "    print(f\"   • Reconstruction errors effectively identify anomalies\")\n",
    "    print(f\"   • Threshold selection critically impacts detection performance\")\n",
    "    print(f\"   • Latent space visualization reveals learned representations\")\n",
    "\n",
    "    print(f\"\\n🚀 Next Steps for Advanced Analysis:\")\n",
    "    print(f\"   • Experiment with different autoencoder architectures\")\n",
    "    print(f\"   • Try variational autoencoders (VAEs) for uncertainty quantification\")\n",
    "    print(f\"   • Implement online anomaly detection for real-time monitoring\")\n",
    "    print(\n",
    "        f\"   • Compare with other unsupervised methods (Isolation Forest, One-Class SVM)\"\n",
    "    )\n",
    "\n",
    "    # Store final results for potential further analysis\n",
    "    final_results = {\n",
    "        \"autoencoder\": autoencoder,\n",
    "        \"detector\": detector,\n",
    "        \"metrics\": metrics,\n",
    "        \"data_info\": data_info,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"normal_scaled\": normal_scaled,\n",
    "        \"anomaly_scaled\": anomaly_scaled,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n💾 Results stored in 'final_results' for further analysis\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Analysis incomplete - please run all previous cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ADVANCED LATENT SPACE ANALYSIS (OPTIONAL)\n",
    "# ============================================================\n",
    "\n",
    "print(\"🌌 Advanced Latent Space Analysis\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "if \"final_results\" in locals() and final_results is not None:\n",
    "\n",
    "    # Extract components from results\n",
    "    autoencoder_model = final_results[\"autoencoder\"]\n",
    "    normal_data = final_results[\"normal_scaled\"]\n",
    "    anomaly_data = final_results[\"anomaly_scaled\"]\n",
    "\n",
    "    print(\"🧠 Extracting and Analyzing Latent Representations\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    try:\n",
    "        # Create encoder model to extract latent representations\n",
    "        encoder_input = autoencoder_model.model.input\n",
    "        encoder_output = autoencoder_model.model.get_layer(\"latent\").output\n",
    "        encoder = tf.keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
    "\n",
    "        # Extract latent representations\n",
    "        print(\"🔵 Computing latent representations for normal data...\")\n",
    "        normal_latent = encoder.predict(\n",
    "            normal_data[:500], verbose=0\n",
    "        )  # Sample for efficiency\n",
    "\n",
    "        print(\"🔴 Computing latent representations for anomaly data...\")\n",
    "        anomaly_latent = encoder.predict(\n",
    "            anomaly_data[:200], verbose=0\n",
    "        )  # Sample for efficiency\n",
    "\n",
    "        print(f\"📊 Latent space analysis:\")\n",
    "        print(f\"   • Normal latent shape: {normal_latent.shape}\")\n",
    "        print(f\"   • Anomaly latent shape: {anomaly_latent.shape}\")\n",
    "        print(f\"   • Latent dimension: {normal_latent.shape[1]}\")\n",
    "\n",
    "        # Combine for analysis\n",
    "        all_latent = np.vstack([normal_latent, anomaly_latent])\n",
    "        labels = np.concatenate(\n",
    "            [\n",
    "                np.zeros(len(normal_latent)),  # 0 for normal\n",
    "                np.ones(len(anomaly_latent)),  # 1 for anomaly\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Latent space statistics\n",
    "        normal_mean = np.mean(normal_latent, axis=0)\n",
    "        anomaly_mean = np.mean(anomaly_latent, axis=0)\n",
    "        latent_separation = np.linalg.norm(normal_mean - anomaly_mean)\n",
    "\n",
    "        print(f\"\\n Latent Space Statistics:\")\n",
    "        print(f\"   • Normal latent mean magnitude: {np.linalg.norm(normal_mean):.3f}\")\n",
    "        print(f\"   • Anomaly latent mean magnitude: {np.linalg.norm(anomaly_mean):.3f}\")\n",
    "        print(f\"   • Mean separation distance: {latent_separation:.3f}\")\n",
    "\n",
    "        # Simple 2D visualization if latent dimension allows\n",
    "        if normal_latent.shape[1] >= 2:\n",
    "            plt.figure(figsize=(12, 5))\n",
    "\n",
    "            # Plot first two latent dimensions\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.scatter(\n",
    "                normal_latent[:, 0],\n",
    "                normal_latent[:, 1],\n",
    "                alpha=0.6,\n",
    "                label=\"Normal\",\n",
    "                color=\"blue\",\n",
    "                s=20,\n",
    "            )\n",
    "            plt.scatter(\n",
    "                anomaly_latent[:, 0],\n",
    "                anomaly_latent[:, 1],\n",
    "                alpha=0.6,\n",
    "                label=\"Anomaly\",\n",
    "                color=\"red\",\n",
    "                s=20,\n",
    "            )\n",
    "            plt.xlabel(\"Latent Dimension 1\")\n",
    "            plt.ylabel(\"Latent Dimension 2\")\n",
    "            plt.title(\"Latent Space (First 2 Dimensions)\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            # Plot latent dimension distributions\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.hist(\n",
    "                normal_latent[:, 0],\n",
    "                bins=30,\n",
    "                alpha=0.7,\n",
    "                label=\"Normal (Dim 1)\",\n",
    "                color=\"blue\",\n",
    "                density=True,\n",
    "            )\n",
    "            plt.hist(\n",
    "                anomaly_latent[:, 0],\n",
    "                bins=30,\n",
    "                alpha=0.7,\n",
    "                label=\"Anomaly (Dim 1)\",\n",
    "                color=\"red\",\n",
    "                density=True,\n",
    "            )\n",
    "            plt.xlabel(\"Latent Value\")\n",
    "            plt.ylabel(\"Density\")\n",
    "            plt.title(\"Latent Dimension 1 Distribution\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        print(f\"\\n✅ Advanced latent space analysis completed\")\n",
    "        print(f\"   • The latent space shows how the autoencoder compresses\")\n",
    "        print(f\"     time series data into a lower-dimensional representation\")\n",
    "        print(f\"   • Separation in latent space indicates learned differences\")\n",
    "        print(f\"     between normal and anomalous patterns\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Advanced latent analysis failed: {str(e)}\")\n",
    "        print(f\"   • This is optional and doesn't affect the main results\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No results available for latent space analysis\")\n",
    "    print(\"   • Please run all previous cells to generate results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c07b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌳 Individual Algorithm Training Example\n",
      "==================================================\n",
      "📊 Comprehensive classification already completed above!\n",
      "🔍 Here's how to access individual algorithm results:\n",
      "\n",
      "🌳 Tree-Based Models Performance:\n",
      "----------------------------------------\n",
      "Decision Tree:\n",
      "   • Training Accuracy: 0.657\n",
      "   • Test Accuracy: 0.389\n",
      "   • Training Time: 3.906s\n",
      "\n",
      "Random Forest:\n",
      "   • Training Accuracy: 0.866\n",
      "   • Test Accuracy: 0.531\n",
      "   • Training Time: 2.421s\n",
      "   • Top 5 Most Important Features:\n",
      "     1. Feature 593: 0.0087\n",
      "     2. Feature 577: 0.0072\n",
      "     3. Feature 581: 0.0065\n",
      "     4. Feature 595: 0.0063\n",
      "     5. Feature 569: 0.0062\n",
      "\n",
      "💡 To train individual algorithms separately:\n",
      "   1. Use supervised_classifier.prepare_data() to get X_train, y_train, X_test, y_test\n",
      "   2. Call supervised_classifier.train_decision_trees(X_train, y_train, X_test, y_test)\n",
      "   3. Or use supervised_classifier.train_svm() or train_neural_networks()\n",
      "\n",
      "🔧 Example: Training only Decision Trees individually\n",
      "(This would be useful if you only want specific algorithms)\n",
      "✅ Data preparation, class balancing, and augmentation already handled by module\n",
      "✅ All models already trained - see comprehensive results above\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LSTM AUTOENCODER FOR NOVELTY DETECTION\n",
    "# ============================================================\n",
    "\n",
    "print(\"🤖 LSTM Autoencoder Novelty Detection Implementation\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Check if we have loaded data from previous cell\n",
    "if (\n",
    "    \"normal_windows\" in locals()\n",
    "    and normal_windows is not None\n",
    "    and len(normal_windows) > 0\n",
    "    and \"anomaly_windows\" in locals()\n",
    "    and anomaly_windows is not None\n",
    "    and len(anomaly_windows) > 0\n",
    "):\n",
    "\n",
    "    # Import required libraries\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import (\n",
    "        Input,\n",
    "        LSTM,\n",
    "        Dense,\n",
    "        RepeatVector,\n",
    "        TimeDistributed,\n",
    "        Dropout,\n",
    "    )\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "    import time\n",
    "\n",
    "    print(\"📊 Using optimized LSTM Autoencoder for novelty detection...\")\n",
    "    print(f\"🟢 Normal operation windows for training: {len(normal_windows)}\")\n",
    "    print(f\"🔴 Anomaly windows for testing: {len(anomaly_windows)}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # SMART DATA SAMPLING FOR FASTER TRAINING\n",
    "    # ============================================================\n",
    "    print(\"\\n⚡ Smart Data Sampling for Training Efficiency\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Use subset for faster training (configurable)\n",
    "    MAX_TRAINING_SAMPLES = 1500  # Reduced for faster training\n",
    "    MAX_ANOMALY_SAMPLES = 500  # Reduced for faster evaluation\n",
    "\n",
    "    print(f\"🎯 Training optimization settings:\")\n",
    "    print(f\"   • Max normal samples for training: {MAX_TRAINING_SAMPLES}\")\n",
    "    print(f\"   • Max anomaly samples for testing: {MAX_ANOMALY_SAMPLES}\")\n",
    "    print(f\"   • This ensures reasonable training time with good performance\")\n",
    "\n",
    "    # Sample normal data for training\n",
    "    if len(normal_windows) > MAX_TRAINING_SAMPLES:\n",
    "        print(\n",
    "            f\"📊 Sampling {MAX_TRAINING_SAMPLES} normal windows from {len(normal_windows)} available...\"\n",
    "        )\n",
    "        # Use random sampling to get diverse examples\n",
    "        import random\n",
    "\n",
    "        sampled_indices = random.sample(\n",
    "            range(len(normal_windows)), MAX_TRAINING_SAMPLES\n",
    "        )\n",
    "        sampled_normal_windows = [normal_windows[i] for i in sampled_indices]\n",
    "    else:\n",
    "        print(f\"📊 Using all {len(normal_windows)} normal windows...\")\n",
    "        sampled_normal_windows = normal_windows\n",
    "\n",
    "    # Sample anomaly data for testing\n",
    "    if len(anomaly_windows) > MAX_ANOMALY_SAMPLES:\n",
    "        print(\n",
    "            f\"📊 Sampling {MAX_ANOMALY_SAMPLES} anomaly windows from {len(anomaly_windows)} available...\"\n",
    "        )\n",
    "        sampled_indices = random.sample(\n",
    "            range(len(anomaly_windows)), MAX_ANOMALY_SAMPLES\n",
    "        )\n",
    "        sampled_anomaly_windows = [anomaly_windows[i] for i in sampled_indices]\n",
    "    else:\n",
    "        print(f\"📊 Using all {len(anomaly_windows)} anomaly windows...\")\n",
    "        sampled_anomaly_windows = anomaly_windows\n",
    "\n",
    "    # ============================================================\n",
    "    # DATA PREPARATION WITH PROGRESS FEEDBACK\n",
    "    # ============================================================\n",
    "    print(\"\\n🔧 Preparing Data for LSTM Autoencoder\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    print(\"📊 Converting normal windows to arrays...\", end=\" \")\n",
    "    start_conversion = time.time()\n",
    "\n",
    "    # Convert normal windows to numpy arrays for training\n",
    "    normal_arrays = []\n",
    "    for i, window in enumerate(sampled_normal_windows):\n",
    "        if i % 200 == 0 and i > 0:\n",
    "            print(\n",
    "                f\"\\r📊 Converting normal windows to arrays... {i}/{len(sampled_normal_windows)}\",\n",
    "                end=\"\",\n",
    "            )\n",
    "        # Ensure consistent shape and convert to numpy\n",
    "        window_array = window.values if hasattr(window, \"values\") else window\n",
    "        normal_arrays.append(window_array)\n",
    "\n",
    "    print(\n",
    "        f\"\\r📊 Converting normal windows to arrays... ✅ ({len(normal_arrays)} processed)\"\n",
    "    )\n",
    "\n",
    "    print(\"📊 Converting anomaly windows to arrays...\", end=\" \")\n",
    "    # Convert anomaly windows to numpy arrays for testing\n",
    "    anomaly_arrays = []\n",
    "    for i, window in enumerate(sampled_anomaly_windows):\n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print(\n",
    "                f\"\\r📊 Converting anomaly windows to arrays... {i}/{len(sampled_anomaly_windows)}\",\n",
    "                end=\"\",\n",
    "            )\n",
    "        # Ensure consistent shape and convert to numpy\n",
    "        window_array = window.values if hasattr(window, \"values\") else window\n",
    "        anomaly_arrays.append(window_array)\n",
    "\n",
    "    print(\n",
    "        f\"\\r📊 Converting anomaly windows to arrays... ✅ ({len(anomaly_arrays)} processed)\"\n",
    "    )\n",
    "\n",
    "    normal_data = np.array(normal_arrays)\n",
    "    anomaly_data = np.array(anomaly_arrays)\n",
    "\n",
    "    conversion_time = time.time() - start_conversion\n",
    "    print(f\"⚡ Data conversion completed in {conversion_time:.2f} seconds\")\n",
    "\n",
    "    print(f\"\\n📐 Data shapes:\")\n",
    "    print(f\"   • Normal data: {normal_data.shape}\")\n",
    "    print(f\"   • Anomaly data: {anomaly_data.shape}\")\n",
    "\n",
    "    # Get dimensions\n",
    "    n_normal_samples, time_steps, n_features = normal_data.shape\n",
    "\n",
    "    print(f\"\\n📋 LSTM Autoencoder Configuration:\")\n",
    "    print(f\"   • Time steps per window: {time_steps}\")\n",
    "    print(f\"   • Features per time step: {n_features}\")\n",
    "    print(f\"   • Normal training samples: {n_normal_samples}\")\n",
    "    print(f\"   • Anomaly test samples: {anomaly_data.shape[0]}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # DATA PREPARATION (ALREADY SCALED)\n",
    "    # ============================================================\n",
    "    print(\"\\n📊 Data Already Preprocessed\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    print(\"✅ Using pre-scaled windowed data from data treatment process\")\n",
    "    # Data is already normalized from the windowing process - no additional scaling needed\n",
    "    normal_scaled = normal_data\n",
    "    anomaly_scaled = anomaly_data\n",
    "\n",
    "    print(f\"📏 Data characteristics:\")\n",
    "    print(\n",
    "        f\"   • Normal data range: [{np.min(normal_scaled):.3f}, {np.max(normal_scaled):.3f}]\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   • Anomaly data range: [{np.min(anomaly_scaled):.3f}, {np.max(anomaly_scaled):.3f}]\"\n",
    "    )\n",
    "    print(f\"   • Data already optimized for neural network training\")\n",
    "\n",
    "    # ============================================================\n",
    "    # OPTIMIZED LSTM AUTOENCODER ARCHITECTURE\n",
    "    # ============================================================\n",
    "    print(\"\\n🧠 Building Optimized LSTM Autoencoder Architecture\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Optimized hyperparameters for faster training\n",
    "    latent_dim = 32  # Reduced from 64 for faster training\n",
    "    lstm_units = 64  # Reduced from 128 for faster training\n",
    "\n",
    "    print(f\"🏗️ Optimized Architecture Configuration:\")\n",
    "    print(f\"   • Input shape: ({time_steps}, {n_features})\")\n",
    "    print(f\"   • Encoder LSTM units: {lstm_units} (reduced for speed)\")\n",
    "    print(f\"   • Latent dimension: {latent_dim} (reduced for speed)\")\n",
    "    print(f\"   • Decoder LSTM units: {lstm_units}\")\n",
    "    print(f\"   • Output shape: ({time_steps}, {n_features})\")\n",
    "    print(f\"   • Dropout added for regularization\")\n",
    "\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(time_steps, n_features), name=\"input\")\n",
    "\n",
    "    # Encoder with dropout for regularization\n",
    "    encoded = LSTM(lstm_units, activation=\"relu\", name=\"encoder_lstm\")(input_layer)\n",
    "    encoded = Dropout(0.2, name=\"encoder_dropout\")(encoded)\n",
    "\n",
    "    # Latent representation (bottleneck)\n",
    "    latent = Dense(latent_dim, activation=\"relu\", name=\"latent\")(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = RepeatVector(time_steps, name=\"repeat_vector\")(latent)\n",
    "    decoded = LSTM(\n",
    "        lstm_units, activation=\"relu\", return_sequences=True, name=\"decoder_lstm\"\n",
    "    )(decoded)\n",
    "    decoded = Dropout(0.2, name=\"decoder_dropout\")(decoded)\n",
    "    decoded = TimeDistributed(Dense(n_features), name=\"output\")(decoded)\n",
    "\n",
    "    # Create autoencoder model\n",
    "    autoencoder = Model(input_layer, decoded, name=\"optimized_lstm_autoencoder\")\n",
    "\n",
    "    # Compile model with optimized settings\n",
    "    autoencoder.compile(\n",
    "        optimizer=Adam(\n",
    "            learning_rate=0.002\n",
    "        ),  # Slightly higher learning rate for faster convergence\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"],\n",
    "    )\n",
    "\n",
    "    print(\"✅ Optimized LSTM Autoencoder model created\")\n",
    "\n",
    "    # Display model summary (compact)\n",
    "    print(f\"\\n📋 Model Summary:\")\n",
    "    print(f\"   • Total parameters: {autoencoder.count_params():,}\")\n",
    "    print(\n",
    "        f\"   • Trainable parameters: {sum([tf.keras.backend.count_params(w) for w in autoencoder.trainable_weights]):,}\"\n",
    "    )\n",
    "    print(f\"   • Model architecture optimized for faster training\")\n",
    "\n",
    "    # ============================================================\n",
    "    # OPTIMIZED MODEL TRAINING WITH PROGRESS FEEDBACK\n",
    "    # ============================================================\n",
    "    print(\"\\n🚂 Training Optimized LSTM Autoencoder on Normal Data\")\n",
    "    print(\"=\" * 55)\n",
    "\n",
    "    # Split normal data into train/validation (80/20 split)\n",
    "    split_idx = int(0.8 * len(normal_scaled))\n",
    "    train_normal = normal_scaled[:split_idx]\n",
    "    val_normal = normal_scaled[split_idx:]\n",
    "\n",
    "    print(f\"📊 Training split:\")\n",
    "    print(f\"   • Training samples: {len(train_normal)}\")\n",
    "    print(f\"   • Validation samples: {len(val_normal)}\")\n",
    "\n",
    "    # Optimized callbacks for faster training\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,  # Reduced patience for faster training\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=3, min_lr=0.0001, verbose=1\n",
    "    )\n",
    "\n",
    "    print(f\"🚂 Starting optimized training...\")\n",
    "    print(f\"   • Max epochs: 50 (reduced for speed)\")\n",
    "    print(f\"   • Batch size: 64 (increased for efficiency)\")\n",
    "    print(f\"   • Early stopping patience: 5 epochs\")\n",
    "    print(f\"   • Learning rate reduction on plateau\")\n",
    "\n",
    "    training_start = time.time()\n",
    "\n",
    "    # Train autoencoder with optimized parameters\n",
    "    history = autoencoder.fit(\n",
    "        train_normal,\n",
    "        train_normal,  # Autoencoder: input = target\n",
    "        validation_data=(val_normal, val_normal),\n",
    "        epochs=50,  # Reduced from 100 for faster training\n",
    "        batch_size=64,  # Increased from 32 for faster training\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1,  # Show progress during training\n",
    "    )\n",
    "\n",
    "    training_time = time.time() - training_start\n",
    "    print(f\"\\n✅ Training completed in {training_time:.2f} seconds\")\n",
    "    print(f\"   • Epochs trained: {len(history.history['loss'])}\")\n",
    "    print(f\"   • Final training loss: {history.history['loss'][-1]:.6f}\")\n",
    "    print(f\"   • Final validation loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "\n",
    "    # Quick training visualization\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"loss\"], label=\"Training Loss\", linewidth=2)\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\", linewidth=2)\n",
    "    plt.title(\"Model Loss During Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"mae\"], label=\"Training MAE\", linewidth=2)\n",
    "    plt.plot(history.history[\"val_mae\"], label=\"Validation MAE\", linewidth=2)\n",
    "    plt.title(\"Model MAE During Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Absolute Error\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n📊 Training Summary:\")\n",
    "    print(f\"   ✅ Model successfully trained on normal operation data\")\n",
    "    print(f\"   ✅ Training time: {training_time:.2f} seconds\")\n",
    "    print(f\"   ✅ Architecture optimized for speed and performance\")\n",
    "    print(f\"   ✅ Ready for anomaly detection evaluation\")\n",
    "\n",
    "    print(f\"\\n⚡ Performance Optimizations Applied:\")\n",
    "    print(f\"   • Reduced model complexity (64 LSTM units vs 128)\")\n",
    "    print(f\"   • Smaller latent dimension (32 vs 64)\")\n",
    "    print(f\"   • Smart data sampling for training efficiency\")\n",
    "    print(f\"   • Increased batch size for faster training\")\n",
    "    print(f\"   • Reduced epochs with early stopping\")\n",
    "    print(f\"   • Learning rate scheduling for optimal convergence\")\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"❌ No data available. Please run the previous cell first to load normal and anomaly data.\"\n",
    "    )\n",
    "    autoencoder = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe37606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QUICK AUTOENCODER TEST (OPTIONAL - FOR IMMEDIATE FEEDBACK)\n",
    "# ============================================================\n",
    "\n",
    "print(\"🔬 Quick Autoencoder Test for Immediate Feedback\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# This cell provides immediate feedback to ensure everything works\n",
    "# Run this first if you want to test the setup before full training\n",
    "\n",
    "if (\n",
    "    \"normal_windows\" in locals()\n",
    "    and normal_windows is not None\n",
    "    and len(normal_windows) > 0\n",
    "):\n",
    "\n",
    "    # Quick test with minimal data\n",
    "    print(\"🚀 Testing with minimal dataset for immediate feedback...\")\n",
    "\n",
    "    # Use only a tiny subset for testing\n",
    "    test_normal = normal_windows[:10]  # Just 10 samples for testing\n",
    "    test_anomaly = anomaly_windows[:5] if len(anomaly_windows) >= 5 else anomaly_windows\n",
    "\n",
    "    print(f\"🧪 Test data:\")\n",
    "    print(f\"   • Normal samples: {len(test_normal)}\")\n",
    "    print(f\"   • Anomaly samples: {len(test_anomaly)}\")\n",
    "\n",
    "    # Quick conversion test with class column removal\n",
    "    print(\"📊 Testing data conversion (removing class column)...\", end=\" \")\n",
    "    try:\n",
    "        test_normal_arrays = []\n",
    "        for window in test_normal:\n",
    "            # Get the DataFrame values and remove the class column\n",
    "            if hasattr(window, \"values\"):\n",
    "                window_data = window.copy()\n",
    "                # Remove 'class' column if it exists\n",
    "                if \"class\" in window_data.columns:\n",
    "                    window_data = window_data.drop(\"class\", axis=1)\n",
    "                    print(f\"\\n   📋 Removed 'class' column from DataFrame\")\n",
    "                window_array = window_data.values\n",
    "            else:\n",
    "                # If it's already an array, assume last column is class and remove it\n",
    "                if len(window.shape) == 2 and window.shape[1] > 1:\n",
    "                    window_array = window[:, :-1]  # Remove last column (class)\n",
    "                    print(f\"\\n   📋 Removed last column (assumed class) from array\")\n",
    "                else:\n",
    "                    window_array = window\n",
    "\n",
    "            test_normal_arrays.append(window_array)\n",
    "\n",
    "        test_normal_data = np.array(test_normal_arrays)\n",
    "        print(\"✅\")\n",
    "        print(f\"   • Test normal data shape: {test_normal_data.shape}\")\n",
    "\n",
    "        # Extract dimensions\n",
    "        test_samples, test_time_steps, test_features = test_normal_data.shape\n",
    "        print(f\"   • Time steps: {test_time_steps}\")\n",
    "        print(f\"   • Features: {test_features} (after removing class column)\")\n",
    "\n",
    "        # Check data characteristics\n",
    "        print(\n",
    "            f\"   • Data range: [{np.min(test_normal_data):.3f}, {np.max(test_normal_data):.3f}]\"\n",
    "        )\n",
    "        print(f\"   • Data type: {test_normal_data.dtype}\")\n",
    "\n",
    "        # Verify the first window columns if it's a DataFrame\n",
    "        if hasattr(test_normal[0], \"columns\"):\n",
    "            print(f\"   • Original columns: {list(test_normal[0].columns)}\")\n",
    "            if \"class\" in test_normal[0].columns:\n",
    "                print(\n",
    "                    f\"   • Features after removing class: {list(test_normal[0].drop('class', axis=1).columns)}\"\n",
    "                )\n",
    "\n",
    "        print(f\"\\n✅ Quick test successful! Data is ready for LSTM autoencoder.\")\n",
    "        print(f\"✅ Class column properly handled and removed from features.\")\n",
    "        print(f\"💡 You can now run the full training cell with confidence.\")\n",
    "        print(f\"⚡ Estimated full training time: ~2-5 minutes with optimized settings\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in quick test: {str(e)}\")\n",
    "        print(f\"💡 Please check the data loading cell and try again\")\n",
    "\n",
    "        # Show more detailed error information\n",
    "        import traceback\n",
    "\n",
    "        print(f\"\\n🔍 Detailed error:\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "else:\n",
    "    print(\"❌ No normal_windows data available.\")\n",
    "    print(\"💡 Please run the data loading cell first.\")\n",
    "\n",
    "print(f\"\\n📋 Next Steps:\")\n",
    "print(f\"   1. ✅ Quick test completed - data is compatible\")\n",
    "print(f\"   2. ✅ Class column handling verified\")\n",
    "print(f\"   3. 🚂 Run the full training cell below for complete autoencoder\")\n",
    "print(f\"   4. 🔍 Proceed to anomaly detection evaluation\")\n",
    "print(f\"   5. 📊 Visualize results and performance metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b91df2",
   "metadata": {},
   "source": [
    "##  Introduction to Autoencoders for Novelty Detection (5 minutes)\n",
    "\n",
    "### What are Autoencoders?\n",
    "\n",
    "**Autoencoders** are neural networks designed to learn efficient data representations by training the network to copy its input to its output. They learn to compress and then reconstruct data.\n",
    "\n",
    "### Autoencoder Architecture:\n",
    "- **Input Layer**: Original data (e.g., sensor readings)\n",
    "- **Encoder**: Compresses input into lower-dimensional representation\n",
    "- **Latent Space**: Compressed representation (bottleneck)\n",
    "- **Decoder**: Reconstructs original data from compressed representation\n",
    "- **Output Layer**: Reconstructed data (should match input)\n",
    "\n",
    "### How Autoencoders Detect Novelty:\n",
    "\n",
    "1. **Training Phase**: Learn to reconstruct only normal data\n",
    "2. **Normal Data**: Low reconstruction error (model learned these patterns)\n",
    "3. **Anomalous Data**: High reconstruction error (model never saw these patterns)\n",
    "4. **Threshold**: Statistical boundary between normal and anomalous reconstruction errors\n",
    "\n",
    "### Why Autoencoders for Oil Well Data:\n",
    "\n",
    "#### Advantages:\n",
    "- **Unsupervised Learning**: Only need normal operation data\n",
    "- **Feature Learning**: Automatically discover important sensor patterns\n",
    "- **Dimensionality Reduction**: Handle high-dimensional sensor data efficiently\n",
    "- **Non-linear Patterns**: Capture complex relationships between sensors\n",
    "- **Reconstruction-Based**: Intuitive interpretation of anomaly scores\n",
    "\n",
    "#### LSTM Autoencoders Specifically:\n",
    "- **Temporal Modeling**: Handle time series sensor data naturally\n",
    "- **Sequential Dependencies**: Capture patterns across time steps\n",
    "- **Variable Sequences**: Adapt to different operational phases\n",
    "- **Memory Cells**: Remember long-term normal operation patterns\n",
    "\n",
    "### Novelty Detection Process:\n",
    "\n",
    "1. **Data Preparation**: Normalize and structure time series data\n",
    "2. **Model Training**: Train autoencoder on normal data only\n",
    "3. **Error Computation**: Calculate reconstruction errors for all data\n",
    "4. **Threshold Setting**: Use statistical methods (μ + 3σ) on normal errors\n",
    "5. **Anomaly Detection**: Flag samples with errors above threshold\n",
    "6. **Validation**: Test on known fault data to evaluate performance\n",
    "\n",
    "### Industrial Applications:\n",
    "- **Predictive Maintenance**: Detect equipment degradation early\n",
    "- **Quality Control**: Identify production anomalies\n",
    "- **System Monitoring**: Continuous health assessment\n",
    "- **Safety Systems**: Early warning for critical failures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
